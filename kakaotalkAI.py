import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import time
import re
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import defaultdict, Counter  # Counter ì¶”ê°€
import os
import openai
import networkx as nx
import json


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ë°© ë¶„ì„ê¸°",
    page_icon="ğŸ—’ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
<style>
    .stApp {
        background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
        color: #ffffff;
    }
    .signal-card {
        background: #2d2d2d;
        padding: 20px;
        border-radius: 15px;
        margin: 10px 0;
        border: 1px solid #3d3d3d;
    }
    .high-signal {
        border-left: 5px solid #4CAF50;
    }
    .medium-signal {
        border-left: 5px solid #FFC107;
    }
    .low-signal {
        border-left: 5px solid #f44336;
    }
</style>
""", unsafe_allow_html=True)

# ì‹œê·¸ë„ í¬ì¸íŠ¸ ì •ì˜
SIGNAL_POINTS = {
    "ë¹ ë¥¸ë‹µì¥": {
        "weight": 10,
        "description": "1ë¶„ ì´ë‚´ ë‹µì¥",
        "threshold": 60  # seconds
    },
    "ì´ëª¨í‹°ì½˜": {
        "weight": 5,
        "description": "ì´ëª¨í‹°ì½˜ ì‚¬ìš© ë¹ˆë„",
        "threshold": 0.2  # ë©”ì‹œì§€ë‹¹ ì´ëª¨í‹°ì½˜ ë¹„ìœ¨
    },
    "ì§ˆë¬¸": {
        "weight": 8,
        "description": "ì§ˆë¬¸ ë¹ˆë„",
        "threshold": 0.3  # ë©”ì‹œì§€ë‹¹ ì§ˆë¬¸ ë¹„ìœ¨
    },
    "ë‹µì¥ê¸¸ì´": {
        "weight": 7,
        "description": "í‰ê·  ë‹µì¥ ê¸¸ì´",
        "threshold": 20  # ê¸€ì ìˆ˜
    },
    "ë§ì¥êµ¬": {
        "weight": 6,
        "description": "ë§ì¥êµ¬ ë¹ˆë„",
        "threshold": 0.2  # ë©”ì‹œì§€ë‹¹ ë§ì¥êµ¬ ë¹„ìœ¨
    }
}

# ë¶€ì •ì  ì‹œê·¸ë„ ì •ì˜
NEGATIVE_SIGNALS = {
    "ëŠ¦ì€ë‹µì¥": {
        "weight": -5,
        "description": "3ì‹œê°„ ì´ìƒ ë‹µì¥ ì§€ì—°",
        "threshold": 10800  # seconds
    },
    "ë‹¨ë‹µ": {
        "weight": -3,
        "description": "5ê¸€ì ì´í•˜ ë‹µì¥",
        "threshold": 5  # ê¸€ì ìˆ˜
    },
    "í™”ì œì „í™˜": {
        "weight": -4,
        "description": "ê°‘ì‘ìŠ¤ëŸ¬ìš´ í™”ì œ ì „í™˜",
        "threshold": 0.3  # í™”ì œ ì „í™˜ ë¹„ìœ¨
    }
}

def parse_kakao_chat(text: str) -> pd.DataFrame:
    """ì¹´ì¹´ì˜¤í†¡ ì±„íŒ… ë‚´ìš© íŒŒì‹±"""
    lines = text.split('\n')
    chat_data = []
    
    # ì¹´í†¡ ë©”ì‹œì§€ íŒ¨í„´: [ì´ë¦„] [ì‹œê°„] ë©”ì‹œì§€
    message_pattern = r'\[(.*?)\]\s\[(ì˜¤ì „|ì˜¤í›„)\s(\d{1,2}):(\d{2})\]\s(.*)'
    
    for line in lines:
        match = re.search(message_pattern, line)
        if match:
            name = match.group(1)
            am_pm = match.group(2)
            hour = int(match.group(3))
            minute = int(match.group(4))
            message = match.group(5)
            
            # ì‹œê°„ ë³€í™˜
            if am_pm == "ì˜¤í›„" and hour != 12:
                hour += 12
            elif am_pm == "ì˜¤ì „" and hour == 12:
                hour = 0
                
            # í˜„ì¬ ë‚ ì§œ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë‚ ì§œë„ íŒŒì‹±í•´ì•¼ í•¨)
            timestamp = datetime.now().replace(
                hour=hour, 
                minute=minute,
                second=0,
                microsecond=0
            )
            
            chat_data.append({
                'timestamp': timestamp,
                'name': name,
                'message': message
            })
    
    if not chat_data:
        st.error("ì±„íŒ… ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return pd.DataFrame(columns=['timestamp', 'name', 'message'])
        
    return pd.DataFrame(chat_data)
    
def create_time_pattern(df: pd.DataFrame, target_names: list, my_name: str):
    """ì‹œê°„ëŒ€ë³„ ëŒ€í™” íŒ¨í„´ ë¶„ì„"""
    df['hour'] = df['timestamp'].dt.hour
    
    fig = go.Figure()
    
    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì •ì˜
    colors = ['#ff4b6e', '#ff9eaf', '#ffb4c2', '#ffc9d3']  # ë¶„í™ê³„ì—´
    
    # ê° ëŒ€ìƒìë³„ ëŒ€í™” íŒ¨í„´
    for idx, target_name in enumerate(target_names):
        target_counts = df[df['name'] == target_name].groupby('hour').size()
        # ì—†ëŠ” ì‹œê°„ëŒ€ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
        target_counts = target_counts.reindex(range(24), fill_value=0)
        
        fig.add_trace(go.Bar(
            x=target_counts.index,
            y=target_counts.values,
            name=f"{target_name}ë‹˜ì˜ ëŒ€í™”",
            marker_color=colors[idx % len(colors)]  # ìƒ‰ìƒ ìˆœí™˜
        ))
    
    # ë‚´ ëŒ€í™” íŒ¨í„´
    my_counts = df[df['name'] == my_name].groupby('hour').size()
    # ì—†ëŠ” ì‹œê°„ëŒ€ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
    my_counts = my_counts.reindex(range(24), fill_value=0)
    
    fig.add_trace(go.Bar(
        x=my_counts.index,
        y=my_counts.values,
        name="ë‚˜ì˜ ëŒ€í™”",
        marker_color='#4a90e2'  # íŒŒë€ìƒ‰
    ))
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title=dict(
            text="ì‹œê°„ëŒ€ë³„ ëŒ€í™” íŒ¨í„´ ë¹„êµ",
            font=dict(size=24, color='white')
        ),
        xaxis=dict(
            title="ì‹œê°„",
            ticktext=['ì˜¤ì „ 12ì‹œ', 'ì˜¤ì „ 3ì‹œ', 'ì˜¤ì „ 6ì‹œ', 'ì˜¤ì „ 9ì‹œ', 
                     'ì˜¤í›„ 12ì‹œ', 'ì˜¤í›„ 3ì‹œ', 'ì˜¤í›„ 6ì‹œ', 'ì˜¤í›„ 9ì‹œ'],
            tickvals=[0, 3, 6, 9, 12, 15, 18, 21],
            tickangle=45,
            gridcolor='rgba(255,255,255,0.1)',
            tickfont=dict(color='white')
        ),
        yaxis=dict(
            title="ë©”ì‹œì§€ ìˆ˜",
            gridcolor='rgba(255,255,255,0.1)',
            tickfont=dict(color='white')
        ),
        barmode='group',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color='white'),
        showlegend=True,
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.01,
            bgcolor='rgba(0,0,0,0)',
            font=dict(color='white')
        ),
        margin=dict(t=50, l=50, r=50, b=50)
    )
    
    # xì¶• ê²©ì ì¶”ê°€
    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(255,255,255,0.1)')
    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(255,255,255,0.1)')
    
    return fig


def analyze_signals(df: pd.DataFrame, target_name: str) -> dict:
    """ì±„íŒ… ì‹œê·¸ë„ ë¶„ì„"""
    signals = {}
    
    # íƒ€ê²Ÿì˜ ë©”ì‹œì§€ë§Œ í•„í„°ë§
    target_msgs = df[df['name'] == target_name]
    
    # 1. ë‹µì¥ ì‹œê°„ ë¶„ì„
    df['time_diff'] = df['timestamp'].diff().dt.total_seconds()
    target_response_times = df[df['name'] == target_name]['time_diff'].dropna()
    
    signals['ë¹ ë¥¸ë‹µì¥'] = {
        'score': len(target_response_times[target_response_times < SIGNAL_POINTS['ë¹ ë¥¸ë‹µì¥']['threshold']]) / len(target_response_times) * 100,
        'detail': f"1ë¶„ ì´ë‚´ ë‹µì¥ ë¹„ìœ¨: {len(target_response_times[target_response_times < 60]) / len(target_response_times):.1%}"
    }
    
    # 2. ì´ëª¨í‹°ì½˜ ì‚¬ìš© ë¶„ì„
    emoji_pattern = re.compile(r'[^\w\s,.]')
    emoji_count = target_msgs['message'].apply(lambda x: len(emoji_pattern.findall(x))).sum()
    signals['ì´ëª¨í‹°ì½˜'] = {
        'score': (emoji_count / len(target_msgs)) * 100,
        'detail': f"ë©”ì‹œì§€ë‹¹ ì´ëª¨í‹°ì½˜: {emoji_count / len(target_msgs):.1f}ê°œ"
    }
    
    # 3. ë‹µì¥ ê¸¸ì´ ë¶„ì„
    avg_length = target_msgs['message'].str.len().mean()
    signals['ë‹µì¥ê¸¸ì´'] = {
        'score': min((avg_length / SIGNAL_POINTS['ë‹µì¥ê¸¸ì´']['threshold']) * 100, 100),
        'detail': f"í‰ê·  ë‹µì¥ ê¸¸ì´: {avg_length:.1f}ì"
    }
    
    # 4. ì§ˆë¬¸ ë¶„ì„
    question_pattern = re.compile(r'[?ï¼Ÿ]')
    question_count = target_msgs['message'].apply(lambda x: bool(question_pattern.search(x))).sum()
    signals['ì§ˆë¬¸'] = {
        'score': (question_count / len(target_msgs)) * 100,
        'detail': f"ì§ˆë¬¸ ë¹„ìœ¨: {question_count / len(target_msgs):.1%}"
    }
    
    return signals

def calculate_interest_score(signals: dict) -> float:
    """í˜¸ê°ë„ ì ìˆ˜ ê³„ì‚°"""
    total_weight = sum(SIGNAL_POINTS[k]['weight'] for k in signals.keys())
    weighted_score = sum(
        SIGNAL_POINTS[k]['weight'] * signals[k]['score'] 
        for k in signals.keys()
    )
    return min(weighted_score / total_weight, 100)

def create_time_pattern_plot(df: pd.DataFrame, target_name: str):
    """ì‹œê°„ëŒ€ë³„ ëŒ€í™” íŒ¨í„´ ì‹œê°í™”"""
    df['hour'] = df['timestamp'].dt.hour
    hourly_counts = df[df['name'] == target_name].groupby('hour').size()
    
    fig = go.Figure(data=go.Bar(
        x=hourly_counts.index,
        y=hourly_counts.values,
        marker_color='#ff4b6e'
    ))
    
    fig.update_layout(
        title="ì‹œê°„ëŒ€ë³„ ëŒ€í™” ë¹ˆë„",
        xaxis_title="ì‹œê°„",
        yaxis_title="ë©”ì‹œì§€ ìˆ˜",
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color='white')
    )
    
    return fig

def calculate_avg_response_time(df: pd.DataFrame, target_name: str, my_name: str) -> float:
    """í‰ê·  ë‹µì¥ ì‹œê°„ ê³„ì‚° (ë¶„ ë‹¨ìœ„)"""
    df = df.sort_values('timestamp')
    response_times = []
    
    for i in range(1, len(df)):
        if df.iloc[i]['name'] == target_name and df.iloc[i-1]['name'] == my_name:
            time_diff = (df.iloc[i]['timestamp'] - df.iloc[i-1]['timestamp']).total_seconds() / 60
            if time_diff < 60:  # 1ì‹œê°„ ì´ë‚´ì˜ ë‹µì¥ë§Œ ê³„ì‚°
                response_times.append(time_diff)
    
    return np.mean(response_times) if response_times else 0

def calculate_response_times(df: pd.DataFrame, target_name: str, my_name: str) -> list:
    """ëª¨ë“  ë‹µì¥ ì‹œê°„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì´ˆ ë‹¨ìœ„)"""
    df = df.sort_values('timestamp')
    response_times = []
    
    for i in range(1, len(df)):
        if df.iloc[i]['name'] == target_name and df.iloc[i-1]['name'] == my_name:
            time_diff = (df.iloc[i]['timestamp'] - df.iloc[i-1]['timestamp']).total_seconds()
            if time_diff < 3600:  # 1ì‹œê°„ ì´ë‚´ì˜ ë‹µì¥ë§Œ í¬í•¨
                response_times.append(time_diff)
    
    return response_times

def create_relationship_graph(relationships_data: dict):
    """í–¥ìƒëœ AI ê¸°ë°˜ ê´€ê³„ ë¶„ì„ê³¼ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™”"""
    try:
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()

        # ëŒ€í™” ê°•ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ë§µ ì •ì˜
        color_scale = px.colors.sequential.Viridis

        # ì—£ì§€ ê°€ì¤‘ì¹˜ì™€ ë…¸ë“œ í¬ê¸° ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
        max_weight = 0
        node_interactions = defaultdict(int)
        edge_info = []

        # ê´€ê³„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë…¸ë“œ ë° ì—£ì§€ ì¶”ê°€
        for person1, connections in relationships_data.items():
            for person2, metrics in connections.items():
                # ìƒí˜¸ì‘ìš© ê°•ë„ ê³„ì‚°
                interaction_strength = (
                    metrics['mentions'] * 2 +  # ì§ì ‘ ì–¸ê¸‰
                    metrics['time_overlap'] * 100 +  # ì‹œê°„ëŒ€ ê²¹ì¹¨
                    metrics.get('consecutive_talks', 0)  # ì—°ì† ëŒ€í™”
                )
                
                if interaction_strength > 0:
                    # ì—£ì§€ ì¶”ê°€
                    G.add_edge(person1, person2, weight=interaction_strength)
                    edge_info.append((person1, person2, interaction_strength))
                    
                    # ë…¸ë“œë³„ ì´ ìƒí˜¸ì‘ìš© ì§‘ê³„
                    node_interactions[person1] += interaction_strength
                    node_interactions[person2] += interaction_strength
                    
                    max_weight = max(max_weight, interaction_strength)

        if not G.edges():
            return go.Figure()

        # ë…¸ë“œ ìœ„ì¹˜ ê³„ì‚° (spring_layoutìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë°°ì¹˜)
        pos = nx.spring_layout(G, k=1, iterations=50)

        # ì—£ì§€ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
        edge_traces = []
        for person1, person2, weight in edge_info:
            x0, y0 = pos[person1]
            x1, y1 = pos[person2]
            
            # ìƒí˜¸ì‘ìš© ê°•ë„ì— ë”°ë¥¸ ì„  ë‘ê»˜ì™€ ìƒ‰ìƒ ì„¤ì •
            width = (weight / max_weight * 8) + 1  # ìµœì†Œ 1, ìµœëŒ€ 9
            color_intensity = weight / max_weight
            color_idx = int(color_intensity * (len(color_scale) - 1))
            color = color_scale[color_idx]

            edge_trace = go.Scatter(
                x=[x0, x1, None],
                y=[y0, y1, None],
                line=dict(
                    width=width,
                    color=color
                ),
                hoverinfo='text',
                text=f"{person1} â†” {person2}<br>ìƒí˜¸ì‘ìš© ê°•ë„: {weight:.1f}",
                mode='lines'
            )
            edge_traces.append(edge_trace)

        # ë…¸ë“œ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
        node_x = []
        node_y = []
        node_text = []
        node_sizes = []
        node_colors = []

        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            
            # ë…¸ë“œ í¬ê¸°ëŠ” ì´ ìƒí˜¸ì‘ìš©ëŸ‰ì— ë¹„ë¡€
            interaction_amount = node_interactions[node]
            size = np.sqrt(interaction_amount) * 2 + 20  # ê¸°ë³¸ í¬ê¸° 20ì— ìƒí˜¸ì‘ìš©ëŸ‰ ë°˜ì˜
            node_sizes.append(size)
            
            # ë…¸ë“œ ìƒ‰ìƒì€ ìƒí˜¸ì‘ìš© ë¹„ìœ¨ì— ë”°ë¼
            color_intensity = interaction_amount / max(node_interactions.values())
            color_idx = int(color_intensity * (len(color_scale) - 1))
            node_colors.append(color_scale[color_idx])
            
            # í˜¸ë²„ í…ìŠ¤íŠ¸ì— ìƒì„¸ ì •ë³´ ì¶”ê°€
            hover_text = f"<b>{node}</b><br>"
            hover_text += f"ì´ ìƒí˜¸ì‘ìš©: {interaction_amount:.0f}<br>"
            hover_text += "<b>ì£¼ìš” ëŒ€í™” ìƒëŒ€:</b><br>"
            
            # ìƒìœ„ 3ëª…ì˜ ëŒ€í™” ìƒëŒ€ ì¶”ê°€
            connections = relationships_data.get(node, {})
            top_connections = sorted(
                connections.items(),
                key=lambda x: (
                    x[1]['mentions'] * 2 +
                    x[1]['time_overlap'] * 100 +
                    x[1].get('consecutive_talks', 0)
                ),
                reverse=True
            )[:3]
            
            for person, metrics in top_connections:
                interaction = (
                    metrics['mentions'] * 2 +
                    metrics['time_overlap'] * 100 +
                    metrics.get('consecutive_talks', 0)
                )
                hover_text += f"- {person}: {interaction:.0f}<br>"
            
            node_text.append(hover_text)

        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            hoverinfo='text',
            text=[node for node in G.nodes()],
            textposition="top center",
            hovertext=node_text,
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=2, color='white')
            )
        )

        # Figure ìƒì„±
        fig = go.Figure(
            data=[*edge_traces, node_trace],
            layout=go.Layout(
                title=dict(
                    text='ëŒ€í™” ì°¸ì—¬ì ê´€ê³„ë„',
                    font=dict(size=24, color='white'),
                    y=0.95
                ),
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20, l=5, r=5, t=40),
                annotations=[dict(
                    text="ğŸ’¡ ì„ ì´ êµµê³  ì§„í• ìˆ˜ë¡ ë” ë§ì€ ëŒ€í™”ë¥¼ ë‚˜ëˆˆ ì‚¬ì´ì…ë‹ˆë‹¤",
                    showarrow=False,
                    xref="paper", yref="paper",
                    x=0.005, y=-0.002,
                    font=dict(size=14, color='white')
                )],
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)'
            )
        )

        # ì»¬ëŸ¬ë°” ì¶”ê°€
        colorbar_trace = go.Scatter(
            x=[None],
            y=[None],
            mode='markers',
            marker=dict(
                colorscale='Viridis',
                showscale=True,
                cmin=0,
                cmax=max_weight,
                colorbar=dict(
                    title='ìƒí˜¸ì‘ìš© ê°•ë„',
                    thickness=15,
                    title_font=dict(color='white'),
                    tickfont=dict(color='white'),
                    xanchor='left',
                    titleside='right'
                )
            ),
            hoverinfo='none'
        )
        fig.add_trace(colorbar_trace)

        return fig

    except Exception as e:
        st.error(f"ê´€ê³„ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return go.Figure()

def analyze_keywords(messages: pd.Series) -> list:
    """ìì£¼ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ë¶„ì„"""
    # ë¶ˆìš©ì–´ ì •ì˜
    ã„´stopwords = set(['ê·¸ë˜ì„œ', 'ë‚˜ëŠ”', 'ì§€ê¸ˆ', 'https', 'www', 'naver', 'com', 'ìƒµê²€ìƒ‰', 'ã…‡ã…‡', 'ë‚´ê°€', 'ë‚˜ë„', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¼', 'ì´ì œ', 'ì €ê¸°', 'ê·¸ê²Œ', 'ìŒ', 'ì•„', 'ì–´', 'ì‘', 'ì´ëª¨í‹°ì½˜', 'ã…‹', 'ã…‹ã…‹', 'ã…‹ã…‹ã…‹', 'ã…‹ã…‹ã…‹ã…‹', 'ã…ã…', 'ã„·ã„·', 'ã…','ì‚¬ì§„', 'ê·¼ë°' , 'ì¼ë‹¨' , 'ì´ì œ', 'ë‹¤ë“¤', 'ì €ê±°' ,'www', 'http', 'youtube', 'ì‚­ì œëœ ë©”ì‹œì§€ì…ë‹ˆë‹¤', 'ê·¸ë¦¬ê³ ', 
                        'ë„¤', 'ì˜ˆ', 'ì•„ì§', 'ìš°ë¦¬', 'ë§ì´', 'ì¡´ë‚˜', 'ã…‹ã…‹ã…‹ã…‹ã…‹', 'ì €ë„', 'ê°™ì€ë°', 'ê·¸ëƒ¥', 'ë„ˆë¬´', 'ì§„ì§œ', 'ë‹¤ì‹œ', 'ì˜¤ëŠ˜', 'ë³´ë©´' 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 'ê·¼ë°', 'ì €ê¸°', 'ì´ê±°', 'ê·¸ê±°', 'ìš”', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼'])
        
    
    # ëª¨ë“  ë©”ì‹œì§€ í•©ì¹˜ê¸°
    text = ' '.join(messages.dropna().astype(str))
    
    # ë‹¨ì–´ ë¶„ë¦¬ ë° ì¹´ìš´íŒ…
    words = text.split()
    word_counts = defaultdict(int)
    
    for word in words:
        if len(word) > 1 and word not in stopwords:  # 2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸
            word_counts[word] += 1
    
    # ìƒìœ„ 20ê°œ ë‹¨ì–´ ë°˜í™˜
    return sorted(word_counts.keys(), key=lambda x: word_counts[x], reverse=True)[:20]

def create_wordcloud(messages: pd.Series) -> plt.Figure:
    """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    try:
        # í°íŠ¸ ì„¤ì¹˜
        os.system('apt-get update && apt-get install fonts-nanum fonts-nanum-coding fonts-nanum-extra -y')
        
        # í°íŠ¸ ê²½ë¡œ ì°¾ê¸°
        font_dirs = [
            '/usr/share/fonts/truetype/nanum',
            '/usr/share/fonts',
            '~/.local/share/fonts',
            '/usr/local/share/fonts'
        ]
        
        font_path = None
        for font_dir in font_dirs:
            if os.path.exists(f"{font_dir}/NanumGothic.ttf"):
                font_path = f"{font_dir}/NanumGothic.ttf"
                break
        
        if not font_path:
            # í°íŠ¸ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
            os.system('wget https://raw.githubusercontent.com/apparition47/NanumGothic/master/NanumGothic.ttf -P /tmp/')
            font_path = '/tmp/NanumGothic.ttf'
        
        # matplotlib í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'NanumGothic'
        
        # ë¶ˆìš©ì–´ ì„¤ì •
        stopwords = set(['ê·¸ë˜ì„œ', 'ë‚˜ëŠ”', 'ì§€ê¸ˆ', 'https', 'www', 'naver', 'com', 'ìƒµê²€ìƒ‰', 'ã…‡ã…‡', 'ë‚´ê°€', 'ë‚˜ë„', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¼', 'ì´ì œ', 'ì €ê¸°', 'ê·¸ê²Œ', 'ìŒ', 'ì•„', 'ì–´', 'ì‘', 'ì´ëª¨í‹°ì½˜', 'ã…‹', 'ã…‹ã…‹', 'ã…‹ã…‹ã…‹', 'ã…‹ã…‹ã…‹ã…‹', 'ã…ã…', 'ã„·ã„·', 'ã…','ì‚¬ì§„', 'ê·¼ë°' , 'ì¼ë‹¨' , 'ì´ì œ', 'ë‹¤ë“¤', 'ì €ê±°' ,'www', 'http', 'youtube', 'ì‚­ì œëœ ë©”ì‹œì§€ì…ë‹ˆë‹¤', 'ê·¸ë¦¬ê³ ', 
                        'ë„¤', 'ì˜ˆ', 'ì•„ì§', 'ìš°ë¦¬', 'ë§ì´', 'ì¡´ë‚˜', 'ã…‹ã…‹ã…‹ã…‹ã…‹', 'ì €ë„', 'ê°™ì€ë°', 'ê·¸ëƒ¥', 'ë„ˆë¬´', 'ì§„ì§œ', 'ë‹¤ì‹œ', 'ì˜¤ëŠ˜', 'ë³´ë©´' 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹', 'ê·¼ë°', 'ì €ê¸°', 'ì´ê±°', 'ê·¸ê±°', 'ìš”', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼'])
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(
            font_path=font_path,
            width=1200,
            height=600,
            background_color='black',
            colormap='Pastel1',
            prefer_horizontal=0.7,
            max_font_size=200,
            min_font_size=10,
            random_state=42,
            stopwords=stopwords,
            min_word_length=2,  # ìµœì†Œ 2ê¸€ì ì´ìƒ
            normalize_plurals=False,
            repeat=False
        )
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = ' '.join(messages.dropna().astype(str))
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud.generate(text)
        
        # ê·¸ë¦¼ ìƒì„±
        fig = plt.figure(figsize=(15, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.tight_layout(pad=0)
        
        return fig
        
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ì°¨íŠ¸ ë°˜í™˜
        fig = plt.figure(figsize=(15, 8))
        plt.text(0.5, 0.5, 'ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\ní•œê¸€ í°íŠ¸ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤', 
                ha='center', va='center')
        plt.axis('off')
        return fig

def analyze_topics(df: pd.DataFrame) -> dict:
    """ì£¼ì œ ë¶„ì„ (ìµœì í™” ë²„ì „)"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
    keywords = {
        'ì¼ìƒ': ['ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ë°¥', 'ë¨¹', 'ì ', 'ì§‘'],
        'ê°ì •': ['ì¢‹ì•„', 'ì‹«ì–´', 'í–‰ë³µ', 'ìŠ¬í¼', 'í™”ë‚˜', 'ì›ƒ'],
        'ì—…ë¬´': ['ì¼', 'íšŒì‚¬', 'ì—…ë¬´', 'ë¯¸íŒ…', 'í”„ë¡œì íŠ¸'],
        'ì·¨ë¯¸': ['ì˜í™”', 'ê²Œì„', 'ìš´ë™', 'ìŒì•…', 'ì±…'],
    }
    
    topic_counts = defaultdict(int)
    
    for msg in df['message']:
        if isinstance(msg, str):
            msg_lower = msg.lower()
            for topic, words in keywords.items():
                if any(word in msg_lower for word in words):
                    topic_counts[topic] += 1
    
    total = sum(topic_counts.values()) or 1
    return {topic: (count / total) * 100 for topic, count in topic_counts.items()}

def analyze_relationships(df: pd.DataFrame) -> dict:
    """í–¥ìƒëœ ê´€ê³„ ë¶„ì„"""
    relationships = defaultdict(lambda: defaultdict(lambda: {
        'mentions': 0,  # ì§ì ‘ ì–¸ê¸‰
        'time_overlap': 0,  # ì‹œê°„ëŒ€ ê²¹ì¹¨
        'consecutive_talks': 0,  # ì—°ì† ëŒ€í™”
        'reaction_rate': 0,  # ë°˜ì‘ë¥ 
        'common_topics': set()  # ê³µí†µ ê´€ì‹¬ì‚¬
    }))
    
    # ë©˜ì…˜ ë¶„ì„ (ì´ë¦„ ì–¸ê¸‰ íšŸìˆ˜)
    names = set(df['name'].unique())
    for _, row in df.iterrows():
        for name in names:
            if name in str(row['message']) and name != row['name']:
                relationships[row['name']][name]['mentions'] += 1

    # ì‹œê°„ëŒ€ ê²¹ì¹¨ ë¶„ì„
    for name1 in names:
        for name2 in names:
            if name1 != name2:
                time_overlap = analyze_time_overlap(df, name1, name2)
                relationships[name1][name2]['time_overlap'] = time_overlap

    # ì—°ì† ëŒ€í™” ë¶„ì„
    df_sorted = df.sort_values('timestamp')
    prev_name = None
    for name in df_sorted['name']:
        if prev_name and name != prev_name:
            relationships[prev_name][name]['consecutive_talks'] += 1
            relationships[name][prev_name]['consecutive_talks'] += 1
        prev_name = name

    # ë°˜ì‘ë¥  ë¶„ì„ (ìƒëŒ€ë°© ë©”ì‹œì§€ì— ëŒ€í•œ ë°˜ì‘ ë¹„ìœ¨)
    for name1 in names:
        for name2 in names:
            if name1 != name2:
                reaction_rate = calculate_reaction_rate(df, name1, name2)
                relationships[name1][name2]['reaction_rate'] = reaction_rate

    # ê³µí†µ ê´€ì‹¬ì‚¬ ë¶„ì„
    topic_patterns = {
        'ê²Œì„': r'ê²Œì„|í”Œë ˆì´|ìºë¦­í„°|ì•„ì´í…œ',
        'ìŒì‹': r'ë§›ìˆ|ë¨¹|ì‹ë‹¹|ì¹´í˜',
        'ì˜í™”/ë“œë¼ë§ˆ': r'ì˜í™”|ë“œë¼ë§ˆ|ë°°ìš°|ë°©ì˜',
        'ìŒì•…': r'ë…¸ë˜|ìŒì•…|ê°€ìˆ˜|ì•¨ë²”',
        'ìš´ë™': r'ìš´ë™|í—¬ìŠ¤|ìš”ê°€|ì‚°ì±…',
        'ì—¬í–‰': r'ì—¬í–‰|ê´€ê´‘|ìˆ™ì†Œ|í•­ê³µ',
        'ì¼/ê³µë¶€': r'ì¼|íšŒì‚¬|ê³µë¶€|í•™êµ'
    }

    for name1 in names:
        person1_msgs = ' '.join(df[df['name'] == name1]['message'].astype(str))
        for name2 in names:
            if name1 != name2:
                person2_msgs = ' '.join(df[df['name'] == name2]['message'].astype(str))
                common_topics = set()
                
                for topic, pattern in topic_patterns.items():
                    if (re.search(pattern, person1_msgs, re.IGNORECASE) and 
                        re.search(pattern, person2_msgs, re.IGNORECASE)):
                        common_topics.add(topic)
                
                relationships[name1][name2]['common_topics'] = common_topics

    return relationships

def calculate_reaction_rate(df: pd.DataFrame, name1: str, name2: str) -> float:
    """ë‘ ì‚¬ìš©ì ê°„ì˜ ë°˜ì‘ë¥  ê³„ì‚°"""
    df = df.sort_values('timestamp')
    
    # 1ë¶„ ì´ë‚´ì˜ ë°˜ì‘ì„ "ë°˜ì‘"ìœ¼ë¡œ ê°„ì£¼
    REACTION_THRESHOLD = 60  # seconds
    
    reactions = 0
    total_messages = 0
    
    for i in range(1, len(df)):
        if df.iloc[i-1]['name'] == name1 and df.iloc[i]['name'] == name2:
            time_diff = (df.iloc[i]['timestamp'] - df.iloc[i-1]['timestamp']).total_seconds()
            if time_diff <= REACTION_THRESHOLD:
                reactions += 1
            total_messages += 1
    
    return reactions / max(total_messages, 1)


def analyze_chat_context(df: pd.DataFrame, target_names: list, my_name: str) -> dict:
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (í–¥ìƒëœ ë²„ì „)"""
    try:
        # 1. ë°ì´í„° ìƒ˜í”Œë§ (ìµœê·¼ 1000ê°œ ë©”ì‹œì§€ë§Œ ë¶„ì„)
        df_sample = df.sort_values('timestamp', ascending=False).head(1000)
        
        # 2. ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            'total_messages': len(df),
            'participants': list(df['name'].unique()),
            'date_range': (df['timestamp'].max() - df['timestamp'].min()).days,
            'topics': analyze_topics(df_sample),
            'relationships': analyze_relationships(df_sample)
        }
        
        # 3. ì‹œê°„ëŒ€ë³„ ëŒ€í™”ëŸ‰ ë¶„ì„
        hourly_stats = df_sample.groupby(df_sample['timestamp'].dt.hour).size()
        peak_hours = hourly_stats[hourly_stats > hourly_stats.mean()].index.tolist()
        
        # 4. ëŒ€í™” ì°¸ì—¬ë„ ë¶„ì„
        participation = df_sample['name'].value_counts().to_dict()
        
        def get_gpt_analysis(text_sample, stats, peak_hours, participation):
            try:
                prompt = f"""
ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ë°©ì„ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ê³¼ í†µê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

[ê¸°ë³¸ ì •ë³´]
- ì´ ë©”ì‹œì§€ ìˆ˜: {stats['total_messages']:,}ê°œ
- ì°¸ì—¬ì ìˆ˜: {len(stats['participants'])}ëª…
- ë¶„ì„ ê¸°ê°„: {stats['date_range']}ì¼
- ì£¼ìš” í™œë™ ì‹œê°„ëŒ€: {', '.join(f'{h}ì‹œ' for h in sorted(peak_hours))}

[ë¶„ì„í•  ëŒ€í™” ë‚´ìš©]
{text_sample}

ë‹¤ìŒ í•­ëª©ë“¤ì„ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ğŸ“‹ ì£¼ìš” ëŒ€í™” ì£¼ì œ ë° í† í”½
- ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì£¼ì œë“¤
- ê° ì£¼ì œë³„ ì£¼ìš” í‚¤ì›Œë“œ
- ëŒ€í™”ì˜ ì „ë°˜ì ì¸ ì„±í–¥ê³¼ ë¶„ìœ„ê¸°

2. ğŸ’­ ëŒ€í™” ë‚´ìš© ìš”ì•½
- ì£¼ìš” ëŒ€í™” íë¦„
- ì¤‘ìš”í•œ ë…¼ì˜ì‚¬í•­ì´ë‚˜ ê²°ì •ì‚¬í•­
- ì¸ìƒì ì¸ ëŒ€í™” í¬ì¸íŠ¸

3. ğŸ‘¥ ëŒ€í™” ì°¸ì—¬ íŒ¨í„´
- ê° ì°¸ì—¬ìì˜ ëŒ€í™” ìŠ¤íƒ€ì¼
- ëŒ€í™” ì£¼ë„ì„±ê³¼ ë°˜ì‘ì„±
- íŠ¹ì§•ì ì¸ ìƒí˜¸ì‘ìš© íŒ¨í„´

4. ğŸŒŸ ëŒ€í™”ë°©ì˜ íŠ¹ì„±
- ëŒ€í™”ë°©ì˜ ì „ë°˜ì ì¸ ì„±ê²©
- ì£¼ëœ ì‚¬ìš© ëª©ì ê³¼ ìš©ë„
- ëŒ€í™”ë°©ë§Œì˜ íŠ¹ë³„í•œ íŠ¹ì§•

ê° í•­ëª©ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ë¶„ì„í•´ì£¼ì„¸ìš”.
ëŒ€í™”ì˜ ë§¥ë½ì„ ì˜ íŒŒì•…í•˜ì—¬ ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""

                messages = [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í™” ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ, ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ]
                
                response = openai.ChatCompletion.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.7,
                    timeout=15
                )
                
                return response.choices[0].message['content']
            except Exception as e:
                st.warning(f"GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return "GPT ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 5. ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        analysis_steps = [
            'ê¸°ë³¸ í†µê³„ ê³„ì‚°',
            'í† í”½ ë¶„ì„',
            'ê´€ê³„ ë¶„ì„',
            'ëŒ€í™” íŒ¨í„´ ë¶„ì„',
            'GPT ë¶„ì„'
        ]
        
        for i, step in enumerate(analysis_steps):
            status_text.text(f'ë¶„ì„ ì¤‘... {step}')
            progress_bar.progress((i + 1) * 20)
            time.sleep(0.1)
        
        # 6. ëŒ€í™” ìƒ˜í”Œ ì¤€ë¹„ (ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ìµœê·¼ 100ê°œ ë©”ì‹œì§€)
        recent_messages = df_sample.sort_values('timestamp').tail(100)
        chat_sample = []
        
        # ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ë©° ìƒ˜í”Œ êµ¬ì„±
        prev_date = None
        for _, msg in recent_messages.iterrows():
            curr_date = msg['timestamp'].strftime('%Y-%m-%d')
            if curr_date != prev_date:
                chat_sample.append(f"\n[{curr_date}]\n")
                prev_date = curr_date
            chat_sample.append(f"{msg['name']}: {msg['message']}")
        
        chat_text = '\n'.join(chat_sample)
        
        # 7. ìµœì¢… ê²°ê³¼ ë°˜í™˜
        analysis_result = {
            **stats,
            'peak_hours': peak_hours,
            'participation': participation,
            'gpt_analysis': get_gpt_analysis(chat_text, stats, peak_hours, participation)
        }
        
        progress_bar.progress(100)
        status_text.text('ë¶„ì„ ì™„ë£Œ!')
        time.sleep(0.5)
        status_text.empty()
        progress_bar.empty()
        
        return analysis_result
        
    except Exception as e:
        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None



def analyze_relationships(df: pd.DataFrame) -> dict:
    """ëŒ€í™” ì°¸ì—¬ìë“¤ ê°„ì˜ ê´€ê³„ ë¶„ì„"""
    relationships = {}
    
    # ê° ì°¸ì—¬ìë³„ ìƒí˜¸ì‘ìš© ë¶„ì„
    for name1 in df['name'].unique():
        relationships[name1] = {}
        for name2 in df['name'].unique():
            if name1 != name2:
                # ì—°ì† ëŒ€í™” íšŸìˆ˜
                consecutive_talks = 0
                # ë©˜ì…˜ íšŸìˆ˜
                mentions = len(df[
                    (df['name'] == name1) & 
                    (df['message'].str.contains(name2, na=False))
                ])
                # ëŒ€í™” ì‹œê°„ëŒ€ ê²¹ì¹¨
                time_overlap = analyze_time_overlap(df, name1, name2)
                
                relationships[name1][name2] = {
                    'mentions': mentions,
                    'time_overlap': time_overlap,
                    'consecutive_talks': consecutive_talks
                }
    
    return relationships

def generate_suggestions(analysis: dict) -> dict:
    """ëŒ€í™” ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ì œì•ˆ ìƒì„±"""
    # ê¸°ë³¸ ì œì•ˆ ì •ì˜
    default_suggestions = {
        'positive': [
            "ëŒ€í™”ê°€ ê¾¸ì¤€íˆ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
            "ê¸°ë³¸ì ì¸ ì˜ˆì˜ë¥¼ ì§€í‚¤ë©° ëŒ€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ì„œë¡œì˜ ì˜ê²¬ì„ ì¡´ì¤‘í•˜ë©° ëŒ€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        ],
        'improvements': [
            "ë” ìì£¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”.",
            "ë‹¤ì–‘í•œ ì£¼ì œë¡œ ëŒ€í™”ë¥¼ í™•ì¥í•´ë³´ì„¸ìš”.",
            "ìƒëŒ€ë°©ì˜ ì´ì•¼ê¸°ì— ë” ì ê·¹ì ìœ¼ë¡œ ë°˜ì‘í•´ë³´ì„¸ìš”."
        ]
    }

    try:
        if not analysis:  # analysisê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
            return default_suggestions

        suggestions = {
            'positive': [],
            'improvements': []
        }
        
        # ëŒ€í™”ëŸ‰ ë¶„ì„
        if 'participants' in analysis and len(analysis['participants']) > 2:
            suggestions['positive'].append("ë‹¤ì–‘í•œ ì°¸ì—¬ìë“¤ê³¼ í™œë°œí•œ ëŒ€í™”ê°€ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        
        # ì£¼ì œ ë‹¤ì–‘ì„± ë¶„ì„
        if 'topics' in analysis and analysis['topics']:
            suggestions['positive'].append("ë‹¤ì–‘í•œ ì£¼ì œë¡œ ëŒ€í™”ê°€ ì§„í–‰ë˜ì–´ ëŒ€í™”ê°€ í’ë¶€í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ê¸ì •ì  í”¼ë“œë°± ì¶”ê°€
        suggestions['positive'].extend([
            "ì •ê¸°ì ìœ¼ë¡œ ëŒ€í™”ê°€ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
            "ì„œë¡œ ì¡´ì¤‘í•˜ëŠ” ëŒ€í™”ê°€ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
            "ì ì ˆí•œ ì´ëª¨í‹°ì½˜ ì‚¬ìš©ìœ¼ë¡œ ê°ì • ì „ë‹¬ì´ ì˜ ë˜ê³  ìˆìŠµë‹ˆë‹¤."
        ])
        
        # ê°œì„  ì œì•ˆ ì¶”ê°€
        suggestions['improvements'].extend([
            "ë” ë§ì€ ì§ˆë¬¸ìœ¼ë¡œ ìƒëŒ€ë°©ì˜ ì´ì•¼ê¸°ë¥¼ ì´ëŒì–´ë‚´ë³´ì„¸ìš”.",
            "ê¸´ ëŒ€í™” ê³µë°±ì´ ìˆì„ ë•ŒëŠ” ê°„ë‹¨í•œ ì¸ì‚¬ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ë³´ì„¸ìš”.",
            "ìƒëŒ€ë°©ì˜ ê´€ì‹¬ì‚¬ì— ëŒ€í•´ ë” ê¹Šì´ ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”.",
            "ì´ëª¨í‹°ì½˜ê³¼ í•¨ê»˜ êµ¬ì²´ì ì¸ ê°ì • í‘œí˜„ì„ í•´ë³´ì„¸ìš”."
        ])
        
        return suggestions if suggestions['positive'] or suggestions['improvements'] else default_suggestions
        
    except Exception as e:
        st.error(f"ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return default_suggestions

def display_suggestions(analysis: dict):
    """AI ì œì•ˆ í‘œì‹œ"""
    st.markdown("## ğŸ’¡ AIì˜ ì œì•ˆ")
    suggestions = generate_suggestions(analysis)
    
    if not suggestions:  # suggestionsê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        suggestions = {
            'positive': ["ëŒ€í™” ë¶„ì„ì— ê¸°ë°˜í•œ ì œì•ˆì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
            'improvements': ["ëŒ€í™” ë¶„ì„ì— ê¸°ë°˜í•œ ê°œì„ ì ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        }

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("### âœ¨ ê¸ì •ì ì¸ í¬ì¸íŠ¸")
        for point in suggestions['positive']:
            st.success(point)

    with col2:
        st.markdown("### ğŸ¯ ê°œì„  í¬ì¸íŠ¸")
        for point in suggestions['improvements']:
            st.warning(point)

def analyze_time_overlap(df: pd.DataFrame, name1: str, name2: str) -> float:
    """ë‘ ì°¸ì—¬ìì˜ ëŒ€í™” ì‹œê°„ëŒ€ ê²¹ì¹¨ ì •ë„ ë¶„ì„"""
    person1_times = df[df['name'] == name1]['timestamp'].dt.hour.value_counts()
    person2_times = df[df['name'] == name2]['timestamp'].dt.hour.value_counts()
    
    overlap = sum(min(person1_times.get(hour, 0), person2_times.get(hour, 0)) 
                 for hour in range(24))
    
    total = sum(max(person1_times.get(hour, 0), person2_times.get(hour, 0)) 
                for hour in range(24))
    
    return overlap / total if total > 0 else 0

def find_highlight_messages(df: pd.DataFrame, target_names: list, my_name: str) -> dict:
    """ì¸ìƒì ì¸ ëŒ€í™” ì°¾ê¸°"""
    try:
        highlights = {
            'emotional_messages': [],
            'discussion_messages': [],
            'quick_responses': []
        }
        
        # ê°ì • í‘œí˜„ì´ í¬í•¨ëœ ë©”ì‹œì§€ ì°¾ê¸°
        emotion_patterns = [
            r'[ã…‹ã…]{2,}',  # ì›ƒìŒ
            r'[ã… ã…œ]{2,}',  # ìŠ¬í””
            r'[!?]{2,}',   # ê°•í•œ ê°ì •
            r'ğŸ˜Š|ğŸ˜„|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|â¤ï¸|ğŸ‘|ğŸ™'  # ì´ëª¨í‹°ì½˜
        ]
        
        for target_name in target_names:
            target_msgs = df[df['name'] == target_name].copy()
            
            # 1. ê°ì •ì´ í’ë¶€í•œ ë©”ì‹œì§€
            for pattern in emotion_patterns:
                emotional = target_msgs[target_msgs['message'].str.contains(pattern, regex=True, na=False)]
                for _, msg in emotional.iterrows():
                    highlights['emotional_messages'].append({
                        'timestamp': msg['timestamp'],
                        'name': msg['name'],
                        'message': msg['message']
                    })
            
            # 2. ê¸´ ë©”ì‹œì§€ (í™œë°œí•œ í† ë¡ )
            long_messages = target_msgs[target_msgs['message'].str.len() > 50]
            for _, msg in long_messages.iterrows():
                highlights['discussion_messages'].append({
                    'timestamp': msg['timestamp'],
                    'name': msg['name'],
                    'message': msg['message']
                })
            
            # 3. ë¹ ë¥¸ ë‹µì¥
            target_msgs['prev_msg_time'] = target_msgs['timestamp'].shift(1)
            target_msgs['response_time'] = (target_msgs['timestamp'] - target_msgs['prev_msg_time']).dt.total_seconds()
            
            quick_responses = target_msgs[
                (target_msgs['response_time'] < 60) &  # 1ë¶„ ì´ë‚´ ë‹µì¥
                (target_msgs['response_time'] > 0)     # ìŒìˆ˜ ì œì™¸
            ]
            
            for _, msg in quick_responses.iterrows():
                highlights['quick_responses'].append({
                    'timestamp': msg['timestamp'],
                    'name': msg['name'],
                    'message': msg['message']
                })
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì„ íƒ
        for category in highlights:
            highlights[category] = sorted(
                highlights[category],
                key=lambda x: x['timestamp'],
                reverse=True
            )[:5]
        
        return highlights
        
    except Exception as e:
        st.error(f"ëŒ€í™” í•˜ì´ë¼ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            'emotional_messages': [],
            'discussion_messages': [],
            'quick_responses': []
        }

def analyze_conversation_stats(df: pd.DataFrame) -> dict:
    """ì°¸ì—¬ìë³„ ëŒ€í™”ëŸ‰ ë¶„ì„"""
    conversation_stats = df.groupby('name').size().to_dict()
    return conversation_stats

def create_conversation_chart(conversation_stats: dict) -> go.Figure:
    """ì°¸ì—¬ìë³„ ëŒ€í™”ëŸ‰ ì‹œê°í™”"""
    fig = go.Figure(data=[
        go.Bar(
            x=list(conversation_stats.keys()),
            y=list(conversation_stats.values()),
            marker_color='#4a90e2'
        )
    ])
    fig.update_layout(
        title="ì°¸ì—¬ìë³„ ëŒ€í™”ëŸ‰",
        xaxis_title="ì°¸ì—¬ì",
        yaxis_title="ë©”ì‹œì§€ ìˆ˜",
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color='white')
    )
    return fig

def analyze_emotions(df: pd.DataFrame) -> dict:
    """ê°ì • ë¶„ì„"""
    # ê°ì • ë¶„ì„ ë¡œì§ êµ¬í˜„
    pass

def create_emotion_wordcloud(df: pd.DataFrame) -> plt.Figure:
    """ê°ì •ë³„ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    # ê°ì •ë³„ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë¡œì§ êµ¬í˜„
    pass

def generate_suggestions(analysis: dict) -> dict:
    """ê°œì„  ì œì•ˆ ìƒì„±"""
    # ê°œì„  ì œì•ˆ ìƒì„± ë¡œì§ êµ¬í˜„
    pass

def create_emotion_chart(emotion_stats: dict) -> go.Figure:
    """ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ì°¨íŠ¸ ìƒì„±"""
    try:
        # ê¸°ë³¸ ê°ì •ì´ ì—†ëŠ” ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
        if not emotion_stats:
            emotion_stats = {
                "ê¸ì •": 0,
                "ì¤‘ë¦½": 0,
                "ë¶€ì •": 0
            }
        
        # ê°ì •ë³„ ìƒ‰ìƒ ë§¤í•‘
        color_map = {
            "ê¸ì •": "#4CAF50",  # ì´ˆë¡
            "ì¤‘ë¦½": "#2196F3",  # íŒŒë‘
            "ë¶€ì •": "#F44336",  # ë¹¨ê°•
            "ê¸°ì¨": "#8BC34A",  # ì—°í•œ ì´ˆë¡
            "ìŠ¬í””": "#9C27B0",  # ë³´ë¼
            "í™”ë‚¨": "#FF5722",  # ì£¼í™©
            "ë†€ëŒ": "#FFEB3B",  # ë…¸ë‘
        }
        
        # ë°ì´í„° ì •ë ¬
        sorted_emotions = dict(sorted(emotion_stats.items(), 
                                    key=lambda x: x[1], 
                                    reverse=True))
        
        # ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        
        # ë°” ì°¨íŠ¸ ì¶”ê°€
        fig.add_trace(go.Bar(
            x=list(sorted_emotions.keys()),
            y=list(sorted_emotions.values()),
            marker_color=[color_map.get(emotion, "#757575") for emotion in sorted_emotions.keys()],
            text=[f"{value:.1f}%" for value in sorted_emotions.values()],
            textposition='auto',
        ))
        
        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            title=dict(
                text="ëŒ€í™” ê°ì • ë¶„ì„",
                font=dict(size=24, color='white'),
                y=0.95
            ),
            xaxis=dict(
                title="ê°ì •",
                tickfont=dict(color='white'),
                showgrid=False
            ),
            yaxis=dict(
                title="ë¹„ìœ¨ (%)",
                tickfont=dict(color='white'),
                gridcolor='rgba(255,255,255,0.1)'
            ),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white'),
            showlegend=False,
            margin=dict(t=50, l=50, r=50, b=50),
            bargap=0.3
        )
        
        # í˜¸ë²„ í…œí”Œë¦¿ ì„¤ì •
        fig.update_traces(
            hovertemplate="<b>%{x}</b><br>" +
                         "ë¹„ìœ¨: %{y:.1f}%<br>" +
                         "<extra></extra>"
        )
        
        return fig
        
    except Exception as e:
        st.error(f"ê°ì • ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return go.Figure()

def analyze_emotions(df: pd.DataFrame) -> dict:
    """ëŒ€í™” ë‚´ìš©ì˜ ê°ì • ë¶„ì„"""
    try:
        # ê°ì • í‚¤ì›Œë“œ ì •ì˜
        emotion_keywords = {
            "ê¸ì •": ["ì¢‹ì•„", "ê°ì‚¬", "í–‰ë³µ", "ê¸°ì˜", "ì‹ ë‚˜", "ìµœê³ ", "ì‚¬ë‘", "ì›ƒ", "ã…‹ã…‹", "ã…ã…", "ğŸ˜Š", "ğŸ˜„", "ğŸ‘"],
            "ë¶€ì •": ["ì‹«ì–´", "ì§œì¦", "í™”ë‚˜", "ìŠ¬í”„", "í˜ë“¤", "ì–´ë ¤", "ë‚˜ë¹ ", "ã… ã… ", "ã…œã…œ", "ğŸ˜¢", "ğŸ˜­", "ğŸ˜¡"],
            "ì¤‘ë¦½": ["ê·¸ë˜", "ìŒ", "ì•„", "ë„¤", "ì‘", "ê¸€ì„", "ê·¸ë ‡", "ì•„í•˜", "í "],
        }
        
        # ì „ì²´ ë©”ì‹œì§€ ìˆ˜
        total_messages = len(df)
        emotion_counts = {emotion: 0 for emotion in emotion_keywords.keys()}
        
        # ê° ë©”ì‹œì§€ì˜ ê°ì • ë¶„ì„
        for message in df['message']:
            if isinstance(message, str):  # ë¬¸ìì—´ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                message = message.lower()  # ì†Œë¬¸ì ë³€í™˜
                for emotion, keywords in emotion_keywords.items():
                    if any(keyword in message for keyword in keywords):
                        emotion_counts[emotion] += 1
        
        # ë¹„ìœ¨ ê³„ì‚°
        emotion_percentages = {
            emotion: (count / total_messages) * 100 
            for emotion, count in emotion_counts.items()
        }
        
        return emotion_percentages
        
    except Exception as e:
        st.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {}

def analyze_chat_topics(messages: pd.Series) -> dict:
    """ëŒ€í™” ì£¼ì œ ë¶„ì„"""
    topics = {
        'ì¼ìƒ': ['ë°¥', 'ë¨¹', 'ì ', 'í”¼ê³¤', 'ì˜í™”', 'ë“œë¼ë§ˆ', 'ì£¼ë§'],
        'ê°ì •': ['ì¢‹ì•„', 'ì‹«', 'ìŠ¬í”„', 'í–‰ë³µ', 'ì›ƒ', 'í˜ë“¤'],
        'ì—…ë¬´': ['íšŒì‚¬', 'ì¼', 'ì—…ë¬´', 'í”„ë¡œì íŠ¸', 'ë¯¸íŒ…'],
        'ì·¨ë¯¸': ['ê²Œì„', 'ìš´ë™', 'ìŒì•…', 'ë…ì„œ', 'ì˜í™”'],
    }
    
    topic_counts = defaultdict(int)
    text = ' '.join(messages.dropna().astype(str))
    
    for topic, keywords in topics.items():
        for keyword in keywords:
            if keyword in text:
                topic_counts[topic] += text.count(keyword)
    
    return dict(topic_counts)

def create_topic_chart(topics_data: dict) -> go.Figure:
    """ì£¼ì œë³„ ëŒ€í™” ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ëŠ” ì°¨íŠ¸ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë ¬ (ê°’ì´ í° ìˆœì„œëŒ€ë¡œ)
        sorted_topics = dict(sorted(topics_data.items(), key=lambda x: x[1], reverse=True))
        
        # ì£¼ì œë³„ ìƒ‰ìƒ ë§¤í•‘
        color_map = {
            'ì¼ìƒ': '#FF9999',  # ë¶„í™ë¹› ë ˆë“œ
            'ê°ì •': '#66B2FF',  # í•˜ëŠ˜ìƒ‰
            'ì—…ë¬´': '#99FF99',  # ì—°í•œ ì´ˆë¡
            'ì·¨ë¯¸': '#FFCC99',  # ì—°í•œ ì£¼í™©
            'ê¸°íƒ€': '#CC99FF'   # ì—°í•œ ë³´ë¼
        }
        
        colors = [color_map.get(topic, '#CCCCCC') for topic in sorted_topics.keys()]
        
        # íŒŒì´ ì°¨íŠ¸ ìƒì„±
        fig = go.Figure(data=[
            go.Pie(
                labels=list(sorted_topics.keys()),
                values=list(sorted_topics.values()),
                hole=0.4,  # ë„ë„› ì°¨íŠ¸ ìŠ¤íƒ€ì¼
                marker=dict(colors=colors),
                textinfo='label+percent',
                textfont=dict(color='white'),
                hovertemplate="<b>%{label}</b><br>" +
                            "ë©”ì‹œì§€ ìˆ˜: %{value}<br>" +
                            "ë¹„ìœ¨: %{percent}<br>" +
                            "<extra></extra>"
            )
        ])
        
        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            title=dict(
                text="ì£¼ì œë³„ ëŒ€í™” ë¶„í¬",
                font=dict(size=24, color='white'),
                y=0.95
            ),
            showlegend=True,
            legend=dict(
                font=dict(color='white'),
                bgcolor='rgba(0,0,0,0)',
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            annotations=[
                dict(
                    text="ëŒ€í™”<br>ì£¼ì œ",
                    x=0.5,
                    y=0.5,
                    font=dict(size=20, color='white'),
                    showarrow=False
                )
            ]
        )
        
        return fig
        
    except Exception as e:
        st.error(f"í† í”½ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ Figure ë°˜í™˜
        return go.Figure()

def create_detailed_wordcloud(messages: pd.Series) -> plt.Figure:
    """ê°ì •ë³„ ìƒ‰ìƒì´ ë‹¤ë¥¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    try:
        text = ' '.join(messages.dropna().astype(str))
        
        # ê°ì •ë³„ ìƒ‰ìƒ ë§¤í•‘
        color_func = lambda word, font_size, position, orientation, random_state=None, **kwargs: (
            '#ff6b6b' if word in ['ì¢‹ì•„', 'í–‰ë³µ', 'ì›ƒ'] else  # ê¸ì •
            '#4ecdc4' if word in ['í™”ì´íŒ…', 'ì‘ì›', 'íŒŒì´íŒ…'] else  # ê²©ë ¤
            '#95a5a6' if word in ['ê·¸ë˜', 'ìŒ', 'ì•„'] else  # ì¤‘ë¦½
            'white'  # ê¸°ë³¸
        )
        
        # ë¦¬ëˆ…ìŠ¤ ê¸°ë³¸ í°íŠ¸ ê²½ë¡œ ì‹œë„
        font_paths = [
            '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # ë‚˜ëˆ”ê³ ë”•
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',  # DejaVu Sans
            None  # ê¸°ë³¸ í°íŠ¸
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°
        font_path = None
        for path in font_paths:
            if path and os.path.exists(path):
                font_path = path
                break
        
        wordcloud = WordCloud(
            width=800, 
            height=400,
            background_color='black',
            color_func=color_func,
            font_path=font_path,  # ì°¾ì€ í°íŠ¸ ì‚¬ìš©
            prefer_horizontal=0.7,
            min_font_size=10,
            max_font_size=100,
            random_state=42
        ).generate(text)
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        plt.tight_layout(pad=0)
        return fig
        
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ìƒˆë¡œìš´ ë¶„ì„ í•¨ìˆ˜ë“¤
def get_favorite_emojis(messages: pd.Series, top_k: int = 3) -> list:
    """ìì£¼ ì‚¬ìš©í•˜ëŠ” ì´ëª¨í‹°ì½˜ ë¶„ì„"""
    emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]+|[\u2600-\u26FF\u2700-\u27BF]')
    emojis = []
    for msg in messages:
        if isinstance(msg, str):
            emojis.extend(emoji_pattern.findall(msg))
    return Counter(emojis).most_common(top_k)

def get_frequent_words(messages: pd.Series, top_k: int = 5) -> list:
    """ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ë¶„ì„ (ë¶ˆìš©ì–´ ì œì™¸)"""
    stopwords = set(['ê·¸ë˜ì„œ', 'ë‚˜ëŠ”', 'ì§€ê¸ˆ', 'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¼', 'ë„¤', 'ì˜ˆ', 'ìŒ', 'ì•„'])
    words = []
    for msg in messages:
        if isinstance(msg, str):
            words.extend([w for w in msg.split() if len(w) > 1 and w not in stopwords])
    return Counter(words).most_common(top_k)

def calculate_conversation_starter_ratio(df: pd.DataFrame, name: str) -> float:
    """ëŒ€í™” ì‹œì‘ ë¹„ìœ¨ ê³„ì‚°"""
    df = df.sort_values('timestamp')
    conversation_gaps = df['timestamp'].diff() > pd.Timedelta(minutes=30)
    conversation_starts = df[conversation_gaps]['name'] == name
    return round(conversation_starts.sum() / conversation_gaps.sum() * 100, 1)

def analyze_emotion_patterns(messages: pd.Series) -> dict:
    """ê°ì • í‘œí˜„ íŒ¨í„´ ë¶„ì„"""
    patterns = {
        'ê¸ì •': r'[ã…‹ã…]{2,}|ğŸ˜Š|ğŸ˜„|ğŸ˜†|â¤ï¸|ğŸ‘|ì¢‹ì•„|ê°ì‚¬|í–‰ë³µ',
        'ë¶€ì •': r'[ã… ã…œ]{2,}|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|ğŸ˜±|ìŠ¬í¼|í˜ë“¤|ì§œì¦',
        'ë†€ëŒ': r'[!?]{2,}|ğŸ˜®|ğŸ˜²|í—|ëŒ€ë°•|ë¯¸ì³¤|ì‹¤í™”',
        'ì• ì •': r'â¤ï¸|ğŸ¥°|ğŸ˜˜|ğŸ’•|ì‚¬ë‘|ë³´ê³ ì‹¶|ê·¸ë¦¬ì›Œ'
    }
    
    emotion_counts = {}
    for emotion, pattern in patterns.items():
        count = sum(1 for msg in messages if isinstance(msg, str) and re.search(pattern, msg))
        if count > 0:
            emotion_counts[emotion] = count
            
    total = sum(emotion_counts.values()) or 1
    return {k: round(v/total * 100, 1) for k, v in emotion_counts.items()}

def analyze_conversation_leadership(df: pd.DataFrame, name: str) -> float:
    """ëŒ€í™” ì£¼ë„ì„± ë¶„ì„"""
    user_msgs = df[df['name'] == name]
    total_msgs = len(df)

    starter_ratio = calculate_conversation_starter_ratio(df, name)
    msg_ratio = len(user_msgs) / total_msgs * 100
    question_pattern = r'[?ï¼Ÿ]'
    question_ratio = sum(user_msgs['message'].str.contains(question_pattern, na=False)) / len(user_msgs) * 100
    
    leadership_score = (starter_ratio + msg_ratio + question_ratio) / 3
    return round(leadership_score, 1)

def analyze_humor_patterns(messages: pd.Series) -> str:
    """ìœ ë¨¸ ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
    humor_patterns = {
        'ì´ëª¨í‹°ì½˜ ìœ ë¨¸': r'[ã…‹ã…]{3,}|ğŸ˜†|ğŸ¤£',
        'ë“œë¦½': r'ë“œë¦½|ê°œê·¸|ë†ë‹´|ì¥ë‚œ',
        'ì¬ì¹˜ìˆëŠ” í‘œí˜„': r'ì›ƒê¸´|ì¬ë°Œ|ì›ƒìŒ|ì¬ì¹˜'
    }
    
    humor_counts = {k: sum(messages.str.contains(v, na=False)) for k, v in humor_patterns.items()}
    total_msgs = len(messages)
    
    if sum(humor_counts.values()) / total_msgs < 0.1:
        return "ìœ ë¨¸ ì‚¬ìš© ì ìŒ"
    
    main_humor = max(humor_counts.items(), key=lambda x: x[1])
    return f"{main_humor[0]} ìœ„ì£¼ì˜ ìœ ë¨¸ ì‚¬ìš© ({main_humor[1]}íšŒ)"

def get_reaction_patterns(df: pd.DataFrame, name: str) -> str:
    """ë°˜ì‘ íŒ¨í„´ ë¶„ì„"""
    user_responses = df[df['name'] == name]
    quick_responses = sum(df['timestamp'].diff().dt.total_seconds() < 60)
    
    if len(user_responses) == 0:
        return "ë°˜ì‘ íŒ¨í„´ ë¶„ì„ ë¶ˆê°€"
    
    patterns = []
    if quick_responses / len(user_responses) > 0.3:
        patterns.append("ë¹ ë¥¸ ë°˜ì‘")
    if sum(user_responses['message'].str.contains(r'[ã…‹ã…]{2,}|[!?]{2,}', na=False)) / len(user_responses) > 0.3:
        patterns.append("ê°ì •ì  ë°˜ì‘")
    if sum(user_responses['message'].str.contains(r'ê·¸ë˜ìš”?|ì •ë§ìš”?|ì§„ì§œìš”?', na=False)) / len(user_responses) > 0.2:
        patterns.append("ê³µê°ì  ë°˜ì‘")
        
    return ", ".join(patterns) if patterns else "ì¼ë°˜ì ì¸ ë°˜ì‘"

def analyze_link_sharing(messages: pd.Series) -> str:
    """ë§í¬ ê³µìœ  ì„±í–¥ ë¶„ì„"""
    link_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    link_count = sum(messages.str.contains(link_pattern, na=False))
    
    if link_count == 0:
        return "ë§í¬ ê³µìœ  ì—†ìŒ"
    elif link_count < 5:
        return f"ê°€ë” ë§í¬ ê³µìœ  ({link_count}íšŒ)"
    else:
        return f"í™œë°œí•œ ì •ë³´ ê³µìœ  ({link_count}íšŒ)"

def analyze_question_patterns(messages: pd.Series) -> str:
    """ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„"""
    question_types = {
        'ì¼ë°˜ ì§ˆë¬¸': r'\?|ê¶ê¸ˆ|ì–´ë•Œ|í• ê¹Œ',
        'ì˜ê²¬ ìš”ì²­': r'ì–´ë–»ê²Œ|ì–´ë–¨ê¹Œ|ê´œì°®ì„ê¹Œ|ì¢‹ì„ê¹Œ',
        'ì •ë³´ ìš”ì²­': r'ë­|ì–¸ì œ|ì–´ë””|ëˆ„êµ¬|ì–¼ë§ˆ'
    }
    
    type_counts = {k: sum(messages.str.contains(v, na=False)) for k, v in question_types.items()}
    total = sum(type_counts.values())
    
    if total == 0:
        return "ì§ˆë¬¸ ì ìŒ"
    
    main_type = max(type_counts.items(), key=lambda x: x[1])
    return f"{main_type[0]} ìœ„ì£¼ ({main_type[1]}íšŒ)"

def analyze_personality_with_gpt(df: pd.DataFrame, name: str) -> dict:
    """GPTë¥¼ í™œìš©í•œ ì‚¬ìš©ì ì„±ê²© ë¶„ì„"""
    try:
        # í•´ë‹¹ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
        user_messages = df[df['name'] == name]['message'].tolist()
        
        # ë¶„ì„ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒ˜í”Œë§ (ìµœê·¼ 100ê°œ)
        sample_size = min(100, len(user_messages))
        message_sample = user_messages[-sample_size:]

        # ê°ì • í‘œí˜„ ë¶„ì„
        emotion_pattern = re.compile(r'[ã…‹ã…]{2,}|[ã… ã…œ]{2,}|[!?]{2,}|ğŸ˜Š|ğŸ˜„|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|â¤ï¸|ğŸ‘|ğŸ™')
        emotion_count = sum(1 for msg in message_sample if isinstance(msg, str) and emotion_pattern.search(msg))
        emotion_ratio = emotion_count / sample_size if sample_size > 0 else 0

        # ì´ëª¨í‹°ì½˜ ì‚¬ìš© ë¶„ì„
        emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]+|[\u2600-\u26FF\u2700-\u27BF]|\[ì´ëª¨í‹°ì½˜\]')
        emoji_count = sum(len(emoji_pattern.findall(str(msg))) for msg in message_sample)
        emoji_ratio = emoji_count / sample_size if sample_size > 0 else 0

        # ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ë¶„ì„
        words = []
        stopwords = {'ê·¸ë˜ì„œ', 'ë‚˜ëŠ”', 'ì§€ê¸ˆ', 'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¼', 'ë„¤', 'ì˜ˆ', 'ìŒ', 'ì•„', 
                    'ì €ë„', 'ê·¼ë°', 'ì €ëŠ”', 'ì œê°€', 'ì¢€', 'ì´ì œ', 'ê·¸ëƒ¥', 'ì§„ì§œ', 'ì•„ë‹ˆ', 'ê·¸ê±´'}
        for msg in message_sample:
            if isinstance(msg, str):
                words.extend([word for word in msg.split() if len(word) > 1 and word not in stopwords])
        word_counter = Counter(words)
        frequent_words = word_counter.most_common(5)

        # ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„
        question_pattern = re.compile(r'[?ï¼Ÿ]|ì–´ë•Œ|í• ê¹Œ|ë­|ì–¸ì œ|ì–´ë””|ëˆ„êµ¬')
        question_count = sum(1 for msg in message_sample if isinstance(msg, str) and question_pattern.search(msg))
        question_ratio = question_count / sample_size if sample_size > 0 else 0

        # ëŒ€í™” ì‹œì‘ ë¹„ìœ¨ ë¶„ì„
        df_sorted = df.sort_values('timestamp')
        conversation_gaps = df_sorted['timestamp'].diff() > pd.Timedelta(minutes=30)
        conversation_starts = df_sorted[conversation_gaps]
        starter_count = sum(conversation_starts['name'] == name)
        starter_ratio = starter_count / len(conversation_starts) if len(conversation_starts) > 0 else 0

        # ë©”ì‹œì§€ ê¸¸ì´ ë¶„ì„
        msg_lengths = [len(str(msg)) for msg in message_sample]
        avg_length = sum(msg_lengths) / len(msg_lengths) if msg_lengths else 0
        
        # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        metrics = {
            "ê°ì •í‘œí˜„ë¹„ìœ¨": round(emotion_ratio * 100, 1),
            "ì´ëª¨í‹°ì½˜ë¹„ìœ¨": round(emoji_ratio * 100, 1),
            "ì§ˆë¬¸ë¹„ìœ¨": round(question_ratio * 100, 1),
            "ëŒ€í™”ì‹œì‘ë¹„ìœ¨": round(starter_ratio * 100, 1),
            "í‰ê· ë©”ì‹œì§€ê¸¸ì´": round(avg_length, 1),
            "ìì£¼ì“°ëŠ”ë‹¨ì–´": frequent_words
        }

        prompt = f"""
ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì‹¬ë¦¬í•™ ë°•ì‚¬ì´ì ëŒ€í™” ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {name}ë‹˜ì˜ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ì„±ê²© ë¶„ì„ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.

[ë¶„ì„ ë°ì´í„°]
1. ëŒ€í™” íŒ¨í„´:
- ëŒ€í™” ì‹œì‘ ë¹„ìœ¨: {metrics['ëŒ€í™”ì‹œì‘ë¹„ìœ¨']}% (ë†’ì„ìˆ˜ë¡ ëŒ€í™” ì£¼ë„ì )
- ì§ˆë¬¸ ë¹„ìœ¨: {metrics['ì§ˆë¬¸ë¹„ìœ¨']}%
- í‰ê·  ë©”ì‹œì§€ ê¸¸ì´: {metrics['í‰ê· ë©”ì‹œì§€ê¸¸ì´']}ì
- ê°ì • í‘œí˜„ ë¹„ìœ¨: {metrics['ê°ì •í‘œí˜„ë¹„ìœ¨']}%
- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ë¹„ìœ¨: {metrics['ì´ëª¨í‹°ì½˜ë¹„ìœ¨']}%

2. ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ (ìƒìœ„ 5ê°œ):
{', '.join(f'{word}({count}íšŒ)' for word, count in metrics['ìì£¼ì“°ëŠ”ë‹¨ì–´'])}

[ì‹¬ì¸µ ë¶„ì„ ìš”ì²­ì‚¬í•­]
1. ğŸ¯ í•µì‹¬ ì„±ê²© íŠ¹ì„± (êµ¬ì²´ì  ê·¼ê±° í•„ìˆ˜)
- ëŒ€í™” ë°ì´í„°ì—ì„œ ë°œê²¬ë˜ëŠ” ê°€ì¥ ë‘ë“œëŸ¬ì§„ ì„±ê²©ì  íŠ¹ì§• 3ê°€ì§€ë¥¼ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ê° íŠ¹ì§•ì´ ëŒ€í™”ì—ì„œ ì–´ë–»ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ëŠ”ì§€ ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ì´ ì‚¬ëŒë§Œì˜ ë…íŠ¹í•œ ë§¤ë ¥ í¬ì¸íŠ¸ë¥¼ ëŒ€í™” ìŠ¤íƒ€ì¼ì—ì„œ ë°œê²¬ë˜ëŠ” íŠ¹ë³„í•œ ì ê³¼ ì—°ê²°ì§€ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”

2. ğŸ—£ï¸ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ ë¶„ì„
- ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ëŠ” íŠ¹ë³„í•œ ë°©ì‹ì´ë‚˜ íŒ¨í„´
- ê°ì •ê³¼ ìƒê°ì„ í‘œí˜„í•  ë•Œì˜ ë…íŠ¹í•œ íŠ¹ì§•
- ê°ˆë“±ì´ë‚˜ ê¸´ì¥ ìƒí™©ì—ì„œì˜ ëŒ€ì²˜ ë°©ì‹
- ìœ ë¨¸ë‚˜ ìœ„íŠ¸ì˜ ì‚¬ìš© íŒ¨í„´ê³¼ ê·¸ íš¨ê³¼

3. ğŸ’ ê´€ê³„ í˜•ì„± ë°©ì‹
- ì¹œë°€ê°ì„ í‘œí˜„í•˜ëŠ” ê³ ìœ í•œ ë°©ë²•
- íƒ€ì¸ì„ ë°°ë ¤í•˜ê±°ë‚˜ ì§€ì§€í•˜ëŠ” íŠ¹ë³„í•œ íŒ¨í„´
- ê·¸ë£¹ ë‚´ì—ì„œì˜ ì—­í• ê³¼ ì˜í–¥ë ¥
- ê´€ê³„ ìœ ì§€ì— ìˆì–´ì„œì˜ ê°•ì 

4. ğŸ­ MBTI ì„±í–¥ ì¶”ì •
- ì™¸í–¥ì„±/ë‚´í–¥ì„± (E/I): {metrics['ëŒ€í™”ì‹œì‘ë¹„ìœ¨']}%ì˜ ëŒ€í™” ì‹œì‘ ë¹„ìœ¨ ë“± ì°¸ê³ 
- ê°ê°/ì§ê´€ (S/N): êµ¬ì²´ì  í‘œí˜„ê³¼ ì¶”ìƒì  í‘œí˜„ì˜ ë¹„ìœ¨ ì°¸ê³ 
- ì‚¬ê³ /ê°ì • (T/F): {metrics['ê°ì •í‘œí˜„ë¹„ìœ¨']}%ì˜ ê°ì • í‘œí˜„ ë¹„ìœ¨ ë“± ì°¸ê³ 
- íŒë‹¨/ì¸ì‹ (J/P): ëŒ€í™” íŒ¨í„´ê³¼ ì‘ë‹µ ìŠ¤íƒ€ì¼ ì°¸ê³ 

5. ğŸ’¡ ì ì¬ë ¥ê³¼ ë°œì „ í¬ì¸íŠ¸
- í˜„ì¬ ê°€ì¥ ì˜ ë°œíœ˜ë˜ê³  ìˆëŠ” ê°•ì 
- ë” ê°œë°œí•˜ë©´ ì¢‹ì„ ì ì¬ì  ì¬ëŠ¥
- ëŒ€ì¸ê´€ê³„ì—ì„œì˜ íŠ¹ë³„í•œ ì˜í–¥ë ¥

ë¶„ì„ì„ í†µí•´ {name}ë‹˜ì˜ ì§„ì •í•œ ë§¤ë ¥ê³¼ íŠ¹ë³„í•œ ê°€ì¹˜ê°€ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ì‹¬ì¸µì ì´ê³  êµ¬ì²´ì ì¸ ë¶„ì„ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹¬ë¦¬í•™ìì´ì ì„±ê²© ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=1500,
            temperature=0.7,
            timeout=15
        )

        return {
            "name": name,
            "metrics": metrics,
            "gpt_analysis": response.choices[0].message['content']
        }
        
    except Exception as e:
        st.error(f"ì„±ê²© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def calculate_personality_metrics(message_text: str) -> dict:
    """ì„±ê²© íŠ¹ì„± ì ìˆ˜ ê³„ì‚°"""
    # ê° íŠ¹ì„±ë³„ íŒ¨í„´ ì •ì˜
    metrics_patterns = {
        "ë§¤ë ¥ë„": {
            "patterns": [
                r'ã…‹ã…‹|ã…ã…|ì›ƒê¸´|ì¬ë¯¸|ì‹ ê¸°|ë©‹ì§€',
                r'ì„¼ìŠ¤|ë°°ë ¤|ì¹œì ˆ|ìƒëƒ¥|ë‹¤ì •|ì°©í•˜',
                r'ğŸ˜Š|ğŸ¤£|ğŸ˜‚|ğŸ˜|ğŸ¥°|ğŸ˜˜|ğŸ˜…|â¤|[ì´ëª¨í‹°ì½˜]'
            ],
            "weight": 1.2
        },
        "ì¹œí™”ë ¥": {
            "patterns": [
                r'ê°™ì´|ìš°ë¦¬|í•¨ê»˜|ì €í¬|ëª¨ë‘|ë‹¤ê°™ì´',
                r'ê³ ë§ˆì›Œ|ê°ì‚¬|ì£„ì†¡|ë¯¸ì•ˆ|ë¶€íƒ|ë„ì™€',
                r'ë§ì•„|ê·¸ë˜|ì‘|ë„¤|ë‹¹ì—°|ê·¸ë ‡ì§€'
            ],
            "weight": 1.1
        },
        "í™œë°œë„": {
            "patterns": [
                r'í•˜ì|ê°€ì|ë†€ì|ë¨¹ì|ë³´ì|í• ê¹Œ',
                r'ì‹ ë‚˜|ì¬ë°Œ|ì¦ê±°|í–‰ë³µ|ì¢‹ì•„|ëŒ€ë°•',
                r'\!+|\?+|ã…‹+|ã…+|~+'
            ],
            "weight": 1.0
        },
        "ê°ì„±ë ¥": {
            "patterns": [
                r'ì¢‹ì•„|í–‰ë³µ|ê·¸ë¦½|ë³´ê³ ì‹¶|ì‚¬ë‘|ì„¤ë ˆ',
                r'ì•„ë¦„|ì˜ˆì˜|ê·€ì—½|ë©‹ì§€|ê·¼ì‚¬|ëŒ€ë‹¨',
                r'ã… ã… |ã…œã…œ|ğŸ˜¢|ğŸ˜­|ğŸ’•|â¤'
            ],
            "weight": 1.0
        },
        "ì§€ì í˜¸ê¸°ì‹¬": {
            "patterns": [
                r'ì™œ|ì–´ë–»ê²Œ|ë¬´ì—‡|ì–¸ì œ|ì–´ë””|ëˆ„êµ¬',
                r'ê´€ì‹¬|ê¶ê¸ˆ|ì•Œê³ |ì‹¶|ì°¾|ê³µë¶€',
                r'ì •ë³´|ì§€ì‹|ì´í•´|í•™ìŠµ|ë°°ìš°|ì—°êµ¬'
            ],
            "weight": 1.1
        }
    }

    scores = {}
    for metric, info in metrics_patterns.items():
        metric_score = 0
        patterns = info["patterns"]
        weight = info["weight"]
        
        for pattern_group in patterns:
            try:
                matches = len(re.findall(pattern_group, message_text, re.IGNORECASE))
                metric_score += min(100, matches * 5)
            except Exception as e:
                print(f"íŒ¨í„´ ë§¤ì¹­ ì˜¤ë¥˜: {pattern_group} - {str(e)}")
                continue
        
        # íŒ¨í„´ ê·¸ë£¹ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ê³„ì‚°
        avg_score = (metric_score / len(patterns)) * weight
        scores[metric] = round(min(100, max(0, avg_score)), 1)
    
    return scores

def analyze_mbti_patterns(messages: pd.Series) -> dict:
    """MBTI ê´€ë ¨ íŒ¨í„´ ë¶„ì„"""
    mbti_indicators = {
        "E": {
            "patterns": [
                r'ê°™ì´|ìš°ë¦¬|ë†€ì|ë§Œë‚˜',
                r'ì¬ë¯¸ìˆ|ì‹ ë‚˜|ì¦ê±°|íŒŒí‹°',
                r'ì‚¬ëŒë“¤|ì¹œêµ¬|ëª¨ì„|ì•½ì†'
            ],
            "counter_patterns": [
                r'í˜¼ì|ì§‘ì—|ì‰¬ê³ |ì¡°ìš©',
                r'í”¼ê³¤|ì§€ì¹œ|í˜ë“¤|ê·€ì°®',
                r'ê°œì¸|ë…ë¦½|ììœ |ì—¬ìœ '
            ]
        },
        "I": {
            "patterns": [
                r'í˜¼ì|ì§‘|ì±…|ìŒì•…',
                r'ì¡°ìš©|í‰í™”|ì‰¬ê³ |ì—¬ìœ ',
                r'ìƒê°|ê³ ë¯¼|ëŠë‚Œ|ë§ˆìŒ'
            ],
            "counter_patterns": [
                r'íŒŒí‹°|ë†€ì|ëª¨ì„|ê°™ì´',
                r'ì‚¬ëŒë“¤|ì¹œêµ¬ë“¤|ìš°ë¦¬|ë‹¤ê°™ì´',
                r'ì‹œëŒ|ë¶ì |ì™ì|ë– ë“¤'
            ]
        },
        "S": {
            "patterns": [
                r'ì§€ê¸ˆ|ì—¬ê¸°|ì˜¤ëŠ˜|ë‚´ì¼',
                r'ì‹¤ì œ|í˜„ì‹¤|ê²½í—˜|ì‚¬ì‹¤',
                r'êµ¬ì²´|ì •í™•|í™•ì‹¤|ì§ì ‘'
            ],
            "counter_patterns": [
                r'ìƒìƒ|ë¯¸ë˜|ê°€ëŠ¥ì„±|ì˜ˆì¸¡',
                r'ì•„ì´ë””ì–´|ì˜ê°|ì§ê°|ëŠë‚Œ',
                r'ì˜ë¯¸|ìƒì§•|ì² í•™|ê´€ê³„'
            ]
        },
        "N": {
            "patterns": [
                r'ìƒìƒ|ì•„ì´ë””ì–´|ì˜ê°|ê°€ëŠ¥',
                r'ì˜ë¯¸|ì´ìœ |ì›ë¦¬|ì´ë¡ ',
                r'ë¯¸ë˜|ë³€í™”|í˜ì‹ |ì°½ì˜'
            ],
            "counter_patterns": [
                r'í˜„ì‹¤|ì‚¬ì‹¤|ê²½í—˜|ì§ì ‘',
                r'êµ¬ì²´|ì •í™•|í™•ì‹¤|ì§€ê¸ˆ',
                r'ì—¬ê¸°|ì˜¤ëŠ˜|ë‚´ì¼|ì‹¤ì œ'
            ]
        },
        "T": {
            "patterns": [
                r'ë…¼ë¦¬|ì´ì„±|ë¶„ì„|íŒë‹¨',
                r'ê°ê´€|ì •í™•|íš¨ìœ¨|ì„±ê³¼',
                r'í•´ê²°|ì›ì¸|ê²°ê³¼|ë°©ë²•'
            ],
            "counter_patterns": [
                r'ê°ì •|ëŠë‚Œ|ë§ˆìŒ|ê³µê°',
                r'ìœ„ë¡œ|ê²©ë ¤|ì§€ì§€|ì‘ì›',
                r'ì¢‹ì•„|ì‹«ì–´|í–‰ë³µ|ìŠ¬í¼'
            ]
        },
        "F": {
            "patterns": [
                r'ê°ì •|ëŠë‚Œ|ë§ˆìŒ|ê³µê°',
                r'ìœ„ë¡œ|ê²©ë ¤|ì§€ì§€|ì‘ì›',
                r'ì‚¬ë‘|í–‰ë³µ|ì¢‹ì•„|ê·¸ë¦¬ì›Œ'
            ],
            "counter_patterns": [
                r'ë…¼ë¦¬|ì´ì„±|ë¶„ì„|íŒë‹¨',
                r'ê°ê´€|ì •í™•|íš¨ìœ¨|ì„±ê³¼',
                r'í•´ê²°|ì›ì¸|ê²°ê³¼|ë°©ë²•'
            ]
        },
        "J": {
            "patterns": [
                r'ê³„íš|ì¼ì •|ì•½ì†|ê·œì¹™',
                r'ì •ë¦¬|ì²´ê³„|ìˆœì„œ|ë‹¨ê³„',
                r'ê²°ì •|í™•ì‹¤|ë§ˆê°|ì™„ë£Œ'
            ],
            "counter_patterns": [
                r'ê°‘ìê¸°|ì¦‰í¥|ììœ |ìœµí†µ',
                r'ë•Œë˜ë©´|ë‚˜ì¤‘|ë¯¸ë£¨|ì–¸ì  ê°€',
                r'ë³€ê²½|ìˆ˜ì •|ìœ ì—°|ì ì‘'
            ]
        },
        "P": {
            "patterns": [
                r'ììœ |ì¦‰í¥|ìœ ì—°|ë³€í™”',
                r'ê°€ëŠ¥ì„±|ì„ íƒ|ëŒ€ì•ˆ|ìœµí†µ',
                r'í¸í•œ|ëŠê¸‹|ì—¬ìœ |ìì—°'
            ],
            "counter_patterns": [
                r'ê³„íš|ì¼ì •|ì•½ì†|ê·œì¹™',
                r'ì •ë¦¬|ì²´ê³„|ìˆœì„œ|ë‹¨ê³„',
                r'ê²°ì •|í™•ì‹¤|ë§ˆê°|ì™„ë£Œ'
            ]
        }
    }

    # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
    msg_text = ' '.join(messages.astype(str))
    mbti_scores = {}
    
    for indicator, patterns in mbti_indicators.items():
        positive_score = sum(len(re.findall(p, msg_text, re.IGNORECASE)) for p in patterns["patterns"])
        negative_score = sum(len(re.findall(p, msg_text, re.IGNORECASE)) for p in patterns["counter_patterns"])
        total_score = positive_score - negative_score
        mbti_scores[indicator] = round(min(100, max(0, 50 + (total_score * 5))), 1)

    # ê° ì§€í‘œìŒì˜ ê°•í•œ ìª½ ì„ íƒ
    trait_pairs = [('E', 'I'), ('S', 'N'), ('T', 'F'), ('J', 'P')]
    predicted_mbti = ''
    probabilities = {}
    
    for pair in trait_pairs:
        score1, score2 = mbti_scores[pair[0]], mbti_scores[pair[1]]
        stronger = pair[0] if score1 > score2 else pair[1]
        confidence = abs(score1 - score2) / 100  # 0~1 ì‚¬ì´ ê°’
        predicted_mbti += stronger
        probabilities[f"{pair[0]}/{pair[1]}"] = round(max(score1, score2), 1)

    return {
        "predicted_type": predicted_mbti,
        "confidence_scores": probabilities,
        "detailed_scores": mbti_scores
    }

def analyze_interests(messages: pd.Series) -> dict:
    """ê´€ì‹¬ì‚¬ ë¶„ì„"""
    interest_categories = {
        "ì—”í„°í…Œì¸ë¨¼íŠ¸": {
            "patterns": [
                r'ì˜í™”|ë“œë¼ë§ˆ|ì˜ˆëŠ¥|ë°©ì†¡',
                r'ìŒì•…|ë…¸ë˜|ê³µì—°|ì¶¤',
                r'ì—°ì˜ˆì¸|ì•„ì´ëŒ|ë°°ìš°|ê°€ìˆ˜'
            ],
            "weight": 1.2
        },
        "ê²Œì„": {
            "patterns": [
                r'ê²Œì„|í”Œë ˆì´|ìºë¦­í„°|ë ™ì—…',
                r'ë¡¤|ë°°ê·¸|ë¡œì•„|ë©”ì´í”Œ',
                r'ê²œ|í‚¬|í´ë¦¬ì–´|ë¯¸ì…˜'
            ],
            "weight": 1.1
        },
        "ìŒì‹": {
            "patterns": [
                r'ë§›ìˆ|ë¨¹|ìŒì‹|ìš”ë¦¬',
                r'ì‹ë‹¹|ì¹´í˜|ë©”ë‰´|ë°°ë‹¬',
                r'ì ì‹¬|ì €ë…|ìˆ |ë””ì €íŠ¸'
            ],
            "weight": 1.0
        },
        "ìš´ë™/ê±´ê°•": {
            "patterns": [
                r'ìš´ë™|í—¬ìŠ¤|ìš”ê°€|í•„ë¼',
                r'ë‹¤ì´ì–´íŠ¸|ê±´ê°•|ì˜ì–‘|ì‹ë‹¨',
                r'ê·¼ìœ¡|ì²´ì¤‘|ìŠ¤íŠ¸ë ˆì¹­|ì‚°ì±…'
            ],
            "weight": 1.1
        },
        "ì—¬í–‰": {
            "patterns": [
                r'ì—¬í–‰|ì—¬í–‰ì§€|ê´€ê´‘|íˆ¬ì–´',
                r'í•´ì™¸|êµ­ë‚´|ë¹„í–‰ê¸°|í˜¸í…”',
                r'í’ê²½|ì‚¬ì§„|êµ¬ê²½|ë°©ë¬¸'
            ],
            "weight": 1.1
        },
        "ë¬¸í™”ìƒí™œ": {
            "patterns": [
                r'ì „ì‹œ|ê³µì—°|ë®¤ì§€ì»¬|ì—°ê·¹',
                r'ì˜í™”ê´€|ê³µì—°ì¥|ë¯¸ìˆ ê´€|ë°•ë¬¼ê´€',
                r'ì±…|ë…ì„œ|ì†Œì„¤|ë§Œí™”'
            ],
            "weight": 1.0
        },
        "ì‡¼í•‘": {
            "patterns": [
                r'ì‡¼í•‘|êµ¬ë§¤|ì§€ë¦„|í• ì¸',
                r'ë¸Œëœë“œ|ì œí’ˆ|ìƒí’ˆ|ì•„ì´í…œ',
                r'ì˜·|ê°€ë°©|ì‹ ë°œ|ì•…ì„¸ì„œë¦¬'
            ],
            "weight": 1.0
        },
        "ì¼/ê³µë¶€": {
            "patterns": [
                r'ì¼|ì—…ë¬´|íšŒì‚¬|í”„ë¡œì íŠ¸',
                r'ê³µë¶€|í•™ìŠµ|ì‹œí—˜|ìê²©ì¦',
                r'ê³¼ì œ|ë¬¸ì œ|ìˆ˜ì—…|ê°•ì˜'
            ],
            "weight": 0.9
        }
    }

    msg_text = ' '.join(messages.astype(str))
    interest_scores = {}
    
    for category, info in interest_categories.items():
        score = 0
        for pattern_group in info["patterns"]:
            matches = sum(len(re.findall(p, msg_text, re.IGNORECASE)) 
                         for p in pattern_group.split('|'))
            score += matches * info["weight"]
        interest_scores[category] = round(min(100, score * 5), 1)

    # Top 3 ê´€ì‹¬ì‚¬ ì¶”ì¶œ
    top_interests = dict(sorted(interest_scores.items(), 
                              key=lambda x: x[1], 
                              reverse=True)[:3])

    return {
        "all_interests": interest_scores,
        "top_interests": top_interests
    }

def calculate_interest_score(messages: list) -> float:
    """ê´€ì‹¬ì‚¬ ì§‘ì¤‘ë„ ì ìˆ˜ ê³„ì‚°"""
    interest_patterns = {
        'ì·¨ë¯¸': r'ì¢‹ì•„|ì¬ë¯¸|ìµœê³ |ëŒ€ë°•|ì§±|ë©‹ì§€',
        'ì—´ì •': r'ì§„ì§œ|ì™„ì „|ë„ˆë¬´|ì •ë§|ë§¤ìš°|ì—„ì²­',
        'ëª°ì…': r'ì°¾ì•„|ë´¤|ì•Œì•„|ë°°ìš°|í•´ë´¤|ë³´ë‹ˆê¹Œ',
        'ì§€ì‹': r'ì•„ëŠ”|ì•Œë ¤|ì„¤ëª…|ì¶”ì²œ|ì •ë³´|í›„ê¸°',
        'ê³µìœ ': r'ë´ë´|ë“¤ì–´ë´|ì•Œë ¤ì¤„ê²Œ|ì¶”ì²œí•´|ê³µìœ '
    }
    return calculate_average_pattern_score(messages, interest_patterns)

def analyze_message_patterns(messages: pd.Series) -> dict:
    """ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„"""
    try:
        if not isinstance(messages, pd.Series):
            messages = pd.Series(messages)
            
        msg_text = ' '.join(messages.dropna().astype(str))
        
        # ê´€ì‹¬ì‚¬ íŒ¨í„´ ì •ì˜
        interest_patterns = {
            'ì—”í„°': [r'ì˜í™”|ë“œë¼ë§ˆ|ì˜ˆëŠ¥|ë°©ì†¡|ìŒì•…|ê³µì—°|ì½˜ì„œíŠ¸'],
            'ìŒì‹': [r'ë§›ìˆ|ë¨¹|ì‹ë‹¹|ì¹´í˜|ë””ì €íŠ¸|ë§›ì§‘|ë°°ë‹¬'],
            'ê²Œì„': [r'ê²Œì„|í”Œë ˆì´|ìºë¦­í„°|ì•„ì´í…œ|ë ™ì—…|ë ˆë²¨'],
            'ìš´ë™': [r'ìš´ë™|í—¬ìŠ¤|ìš”ê°€|í•„ë¼|ì‚°ì±…|ëŸ¬ë‹|ê±·ê¸°'],
            'ì—¬í–‰': [r'ì—¬í–‰|ê´€ê´‘|ìˆ™ì†Œ|í˜¸í…”|ë¹„í–‰|íœ´ê°€|ë†€ëŸ¬'],
            'ì‡¼í•‘': [r'ì‡¼í•‘|êµ¬ë§¤|ì£¼ë¬¸|ë°°ì†¡|ìƒí’ˆ|í• ì¸|ì§€ë¦„'],
            'ê³µë¶€': [r'ê³µë¶€|í•™ìŠµ|ê°•ì˜|ìˆ˜ì—…|ì‹œí—˜|ê³¼ì œ|ìê²©']
        }
        
        # ê° ê´€ì‹¬ì‚¬ë³„ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for topic, patterns in interest_patterns.items():
            pattern_count = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, msg_text, re.IGNORECASE))
                pattern_count += matches
            
            # ë©”ì‹œì§€ ê¸¸ì´ë¡œ ì •ê·œí™”
            normalized_score = min(100, (pattern_count / max(1, len(messages))) * 100)
            if normalized_score > 0:  # ì ìˆ˜ê°€ 0ì¸ ê²½ìš°ëŠ” ì œì™¸
                scores[topic] = round(normalized_score, 1)
        
        # ëŒ€í™” ìŠ¤íƒ€ì¼ ë¶„ì„
        style_patterns = {
            'ì´ëª¨í‹°ì½˜': r'[ğŸ˜ŠğŸ¤£ğŸ˜‚ğŸ˜ğŸ¥°ğŸ˜˜ğŸ˜…â¤ï¸]',
            'ê°ì •í‘œí˜„': r'[ã…‹ã…ã… ã…œ]{2,}',
            'ê°•ì¡°í‘œí˜„': r'[!?]{2,}',
            'ì¹œê·¼í‘œí˜„': r'ã…‡ã…‡|ã„´ã„´|ã…‡ã…‹|ã„¹ã…‡|ã…Šã…Š'
        }
        
        style_scores = {}
        for style, pattern in style_patterns.items():
            matches = sum(len(re.findall(pattern, str(msg), re.IGNORECASE)) for msg in messages)
            normalized_score = min(100, (matches / max(1, len(messages))) * 100)
            if normalized_score > 0:
                style_scores[style] = round(normalized_score, 1)
        
        return {
            'interests': scores,
            'style': style_scores
        }
        
    except Exception as e:
        st.error(f"ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {'interests': {}, 'style': {}}


def find_dominant_patterns(results: dict) -> list:
    """ê°€ì¥ ë‘ë“œëŸ¬ì§„ íŒ¨í„´ ì°¾ê¸°"""
    dominant = []
    for category, scores in results.items():
        if category != "ìš”ì•½":  # ìš”ì•½ ì¹´í…Œê³ ë¦¬ ì œì™¸
            max_pattern = max(scores.items(), key=lambda x: x[1])
            if max_pattern[1] > 50:  # 50ì  ì´ìƒì¸ ê²½ìš°ë§Œ
                dominant.append({
                    "category": category,
                    "pattern": max_pattern[0],
                    "score": max_pattern[1]
                })
    
    return sorted(dominant, key=lambda x: x['score'], reverse=True)[:3]

def analyze_conversation_tendency(results: dict) -> str:
    """ëŒ€í™” ì„±í–¥ ë¶„ì„"""
    ì†Œí†µ_ì ìˆ˜ = sum(results.get('ì†Œí†µ_ë°©ì‹', {}).values()) / 4
    ê°ì •_ì ìˆ˜ = sum(results.get('ê°ì •_í‘œí˜„', {}).values()) / 4
    
    if ì†Œí†µ_ì ìˆ˜ > 70 and ê°ì •_ì ìˆ˜ > 70:
        return "ì ê·¹ì  ê°ì„±í˜•"
    elif ì†Œí†µ_ì ìˆ˜ > 70:
        return "ì ê·¹ì  ì†Œí†µí˜•"
    elif ê°ì •_ì ìˆ˜ > 70:
        return "ê°ì„± í‘œí˜„í˜•"
    elif ì†Œí†µ_ì ìˆ˜ > 50 and ê°ì •_ì ìˆ˜ > 50:
        return "ê· í˜•ì  ì†Œí†µí˜•"
    else:
        return "ì°¨ë¶„í•œ ì†Œí†µí˜•"

def calculate_communication_level(results: dict) -> dict:
    """ì†Œí†µ ë ˆë²¨ ê³„ì‚°"""
    # ê° ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
    category_scores = {
        category: sum(scores.values()) / len(scores)
        for category, scores in results.items()
        if category != "ìš”ì•½"
    }
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    total_score = sum(category_scores.values()) / len(category_scores)
    
    # ë ˆë²¨ ê²°ì •
    if total_score > 80:
        level = "ì†Œí†µ ë§ˆìŠ¤í„°"
    elif total_score > 60:
        level = "ì†Œí†µ ê³ ìˆ˜"
    elif total_score > 40:
        level = "ì†Œí†µ ì¤‘ìˆ˜"
    else:
        level = "ì†Œí†µ ì´ˆë³´"
    
    return {
        "level": level,
        "score": round(total_score, 1)
    }


def create_personality_radar_chart(metrics: dict) -> go.Figure:
    """ì„±ê²© ë¶„ì„ ë ˆì´ë” ì°¨íŠ¸"""
    if not metrics:
        return None
        
    categories = list(metrics.keys())
    values = list(metrics.values())
    
    # ê·€ì—¬ìš´ ì´ëª¨ì§€ì™€ í•¨ê»˜ í‘œì‹œ
    emoji_mapping = {
        "ë§¤ë ¥ë„": "ğŸ’ ë§¤ë ¥ë„",
        "ì¹œí™”ë ¥": "ğŸ¤ ì¹œí™”ë ¥",
        "í™œë°œë„": "âš¡ í™œë°œë„",
        "ê°ì„±ë ¥": "ğŸ’– ê°ì„±ë ¥",
        "ì§€ì í˜¸ê¸°ì‹¬": "ğŸ” ì§€ì í˜¸ê¸°ì‹¬"
    }
    
    emoji_categories = [emoji_mapping.get(cat, cat) for cat in categories]
    
    fig = go.Figure(data=go.Scatterpolar(
        r=values,
        theta=emoji_categories,
        fill='toself',
        marker=dict(color='rgba(255, 105, 180, 0.7)'),
        line=dict(color='rgb(255, 105, 180)'),
        name='ì„±ê²© íŠ¹ì„±'
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickfont=dict(color='white', size=12),
                ticksuffix='%',
                gridcolor='rgba(255, 255, 255, 0.2)'
            ),
            angularaxis=dict(
                tickfont=dict(color='white', size=14),
                gridcolor='rgba(255, 255, 255, 0.2)'
            ),
            bgcolor='rgba(0, 0, 0, 0)'
        ),
        showlegend=False,
        paper_bgcolor='rgba(0, 0, 0, 0)',
        font=dict(color='white'),
        margin=dict(t=30, b=30)
    )
    
    return fig

def display_stat_metrics(title: str, metrics: dict, is_percentage: bool = False):
    """í†µê³„ ë©”íŠ¸ë¦­ì„ ê¹”ë”í•˜ê²Œ í‘œì‹œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    st.markdown(f"**{title}**")
    cols = st.columns(2)
    for idx, (label, value) in enumerate(metrics.items()):
        with cols[idx % 2]:
            container = st.container()
            container.markdown(
                f"""
                <div style="
                    background-color: rgba(255, 255, 255, 0.1);
                    padding: 10px;
                    border-radius: 8px;
                    margin: 5px 0;
                    text-align: center;
                ">
                    <div style="color: rgba(255, 255, 255, 0.7); font-size: 14px;">
                        {label}
                    </div>
                    <div style="color: #FF69B4; font-size: 20px; font-weight: bold;">
                        {value}{"%"if is_percentage and isinstance(value, (int, float)) else ""}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

def display_personality_analysis(df: pd.DataFrame, target_names: list):
    """ì„±ê²© ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    try:
        st.markdown("## ğŸ­ ì„±ê²© ë¶„ì„")
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_messages = len(df)
        analysis_date = df['timestamp'].max().strftime("%Yë…„ %mì›” %dì¼")
        
        st.markdown(f"""
        <div style='text-align: right; color: rgba(255,255,255,0.6); margin-bottom: 20px;'>
            ë¶„ì„ ê¸°ì¤€ì¼: {analysis_date}<br>
            ì´ ë¶„ì„ ë©”ì‹œì§€: {total_messages:,}ê°œ
        </div>
        """, unsafe_allow_html=True)

        for name in target_names:
            with st.spinner(f"{name}ë‹˜ì˜ ì„±ê²© ë¶„ì„ ì¤‘..."):
                analysis = analyze_personality_with_gpt(df, name)
                user_msgs = df[df['name'] == name]
                msg_count = len(user_msgs)
                avg_length = user_msgs['message'].str.len().mean()
                
                # ì‚¬ìš©ì ì¹´ë“œ ì»¨í…Œì´ë„ˆ
                with st.container():
                    st.markdown(f"""
                    <div style="
                        background-color: rgba(45, 45, 45, 0.7);
                        border-radius: 15px;
                        padding: 20px;
                        margin: 20px 0;
                        border: 1px solid rgba(255, 255, 255, 0.1);
                        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                    ">
                        <div style="
                            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
                            margin-bottom: 15px;
                            padding-bottom: 10px;
                        ">
                            <div style="color: #FF69B4; font-size: 24px; font-weight: bold;">
                                ğŸ‘¤ {name}
                            </div>
                            <div style="color: rgba(255, 255, 255, 0.7); font-size: 16px;">
                                ë©”ì‹œì§€ {msg_count:,}ê°œ | í‰ê·  {avg_length:.1f}ì
                            </div>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)

                    # ì£¼ìš” ë¶„ì„ ì»¬ëŸ¼
                    col1, col2 = st.columns([3, 2])
                    
                    with col1:
                        with st.container():
                            st.markdown("### ğŸ¯ AI ì„±ê²© ë¶„ì„")
                            if analysis and "gpt_analysis" in analysis:
                                st.markdown(analysis["gpt_analysis"])
                            else:
                                st.error(f"{name}ë‹˜ì˜ ì„±ê²© ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                    with col2:
                        with st.container():
                            st.markdown("### âœ¨ ì„±ê²© íŠ¹ì„± ì ìˆ˜")
                            if analysis and "patterns" in analysis:
                                metrics = analysis.get("metrics", {})
                                if metrics:
                                    st.plotly_chart(
                                        create_personality_radar_chart(metrics),
                                        use_container_width=True
                                    )
                                    # ì„±ê²© íŠ¹ì„± ì ìˆ˜ í‘œì‹œ
                                    display_stat_metrics("", metrics)
                                else:
                                    st.info("ì„±ê²© íŠ¹ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    # ì¶”ê°€ ë¶„ì„ ì»¬ëŸ¼
                    col3, col4 = st.columns(2)
                    
                    with col3:
                        with st.container():
                            st.markdown("### ğŸ’¬ ëŒ€í™” íŒ¨í„´")
                            response_times = calculate_response_patterns(df, name)
                            if response_times:
                                # ì‘ë‹µ ì‹œê°„ í†µê³„
                                response_stats = {
                                    k: v for k, v in response_times.items() 
                                    if k != "í™œì„±_ì‹œê°„ëŒ€" and isinstance(v, (int, float))
                                }
                                display_stat_metrics("ì‘ë‹µ íŒ¨í„´", response_stats, True)
                                
                                # í™œì„± ì‹œê°„ëŒ€ í‘œì‹œ
                                st.markdown("**í™œë™ ì‹œê°„ëŒ€**")
                                st.markdown(f"""
                                <div style="
                                    background: rgba(255, 105, 180, 0.1);
                                    border-left: 3px solid #FF69B4;
                                    padding: 10px;
                                    margin: 10px 0;
                                    border-radius: 0 8px 8px 0;
                                    font-size: 16px;
                                    color: #FF69B4;
                                ">
                                    {response_times['í™œì„±_ì‹œê°„ëŒ€']}
                                </div>
                                """, unsafe_allow_html=True)

                    with col4:
                        with st.container():
                            st.markdown("### ğŸ¯ ê´€ì‹¬ì‚¬ & ëŒ€í™” ìŠ¤íƒ€ì¼")
                            patterns = analyze_message_patterns(user_msgs['message'])
                            
                            if patterns['interests']:
                                sorted_interests = dict(sorted(
                                    patterns['interests'].items(), 
                                    key=lambda x: x[1], 
                                    reverse=True
                                )[:4])
                                display_stat_metrics("ì£¼ìš” ê´€ì‹¬ì‚¬", sorted_interests, True)
                            
                            if patterns['style']:
                                display_stat_metrics("ëŒ€í™” ìŠ¤íƒ€ì¼", patterns['style'], True)

                    st.markdown("<hr>", unsafe_allow_html=True)

    except Exception as e:
        st.error(f"ì„±ê²© ë¶„ì„ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"Display error: {str(e)}")


def calculate_response_patterns(df: pd.DataFrame, name: str) -> dict:
    """ëŒ€í™” ì‘ë‹µ íŒ¨í„´ ìƒì„¸ ë¶„ì„"""
    try:
        # ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„
        user_df = df[df['name'] == name].copy()
        df_sorted = df.sort_values('timestamp')
        
        # ì‹œê°„ ê°„ê²© ê³„ì‚°
        df_sorted['time_diff'] = df_sorted['timestamp'].diff().dt.total_seconds()
        
        # 1. ì‘ë‹µ ì‹œê°„ ë¶„ì„
        user_responses = df_sorted[
            (df_sorted['name'] == name) & 
            (df_sorted['time_diff'].notna()) & 
            (df_sorted['time_diff'] > 0) & 
            (df_sorted['time_diff'] <= 3600)  # 1ì‹œê°„ ì´ë‚´ ì‘ë‹µë§Œ
        ]
        
        # ë¹ ë¥¸ ì‘ë‹µ (1ë¶„ ì´ë‚´)
        quick_responses = user_responses[user_responses['time_diff'] <= 60]
        # ë³´í†µ ì‘ë‹µ (1ë¶„~5ë¶„)
        normal_responses = user_responses[(user_responses['time_diff'] > 60) & (user_responses['time_diff'] <= 300)]
        # ëŠë¦° ì‘ë‹µ (5ë¶„~1ì‹œê°„)
        slow_responses = user_responses[user_responses['time_diff'] > 300]
        
        total_valid_responses = len(user_responses)
        if total_valid_responses == 0:
            return {
                "í‰ê· _ì‘ë‹µ(ë¶„)": 0,
                "ë¹ ë¥¸_ì‘ë‹µ": 0,
                "ë³´í†µ_ì‘ë‹µ": 0,
                "ëŠë¦°_ì‘ë‹µ": 0,
                "ì‹œê°„ë‹¹_ë©”ì‹œì§€": 0,
                "í™œì„±_ì‹œê°„ëŒ€": "ì—†ìŒ"
            }
        
        # 2. ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„ì„
        hour_dist = user_df['timestamp'].dt.hour.value_counts()
        peak_hours = hour_dist[hour_dist >= hour_dist.mean()].index.tolist()
        peak_hours.sort()
        
        # 3. ë©”ì‹œì§€ ë¹ˆë„ ë¶„ì„
        total_days = (df['timestamp'].max() - df['timestamp'].min()).days + 1
        msgs_per_day = len(user_df) / total_days
        
        # í™œì„± ì‹œê°„ëŒ€ ë¬¸ìì—´ ìƒì„±
        if peak_hours:
            time_ranges = []
            start = peak_hours[0]
            prev = start
            
            for hour in peak_hours[1:] + [peak_hours[0] + 24]:
                if hour != prev + 1:
                    end = prev
                    time_ranges.append(f"{start:02d}~{end:02d}ì‹œ")
                    start = hour
                prev = hour
            
            active_hours = ", ".join(time_ranges)
        else:
            active_hours = "ë¶ˆê·œì¹™"
        
        return {
            "í‰ê· _ì‘ë‹µ(ë¶„)": round(user_responses['time_diff'].mean() / 60, 1),
            "ë¹ ë¥¸_ì‘ë‹µ": round(len(quick_responses) / total_valid_responses * 100, 1),
            "ë³´í†µ_ì‘ë‹µ": round(len(normal_responses) / total_valid_responses * 100, 1),
            "ëŠë¦°_ì‘ë‹µ": round(len(slow_responses) / total_valid_responses * 100, 1),
            "ì‹œê°„ë‹¹_ë©”ì‹œì§€": round(msgs_per_day / 24, 1),
            "í™œì„±_ì‹œê°„ëŒ€": active_hours
        }
        
    except Exception as e:
        st.error(f"ì‘ë‹µ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "í‰ê· _ì‘ë‹µ(ë¶„)": 0,
            "ë¹ ë¥¸_ì‘ë‹µ": 0,
            "ë³´í†µ_ì‘ë‹µ": 0,
            "ëŠë¦°_ì‘ë‹µ": 0,
            "ì‹œê°„ë‹¹_ë©”ì‹œì§€": 0,
            "í™œì„±_ì‹œê°„ëŒ€": "ë¶„ì„ ì‹¤íŒ¨"
        }




def create_personality_radar_chart(metrics: dict) -> go.Figure:
    """ì„±ê²© ë¶„ì„ ë ˆì´ë” ì°¨íŠ¸"""
    categories = list(metrics.keys())
    values = list(metrics.values())
    
    # ê·€ì—¬ìš´ ì´ëª¨ì§€ì™€ í•¨ê»˜ í‘œì‹œ
    emoji_mapping = {
        "ë§¤ë ¥ë„": "ğŸ’ ë§¤ë ¥ë„",
        "ì¹œí™”ë ¥": "ğŸ¤ ì¹œí™”ë ¥",
        "í™œë°œë„": "âš¡ í™œë°œë„",
        "ê°ì„±ë ¥": "ğŸ’– ê°ì„±ë ¥",
        "ì§€ì í˜¸ê¸°ì‹¬": "ğŸ” ì§€ì í˜¸ê¸°ì‹¬"
    }
    
    emoji_categories = [emoji_mapping.get(cat, cat) for cat in categories]
    
    fig = go.Figure(data=go.Scatterpolar(
        r=values,
        theta=emoji_categories,
        fill='toself',
        marker=dict(color='rgba(255, 105, 180, 0.7)'),
        line=dict(color='rgb(255, 105, 180)')
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickfont=dict(color='white', size=12),
                ticksuffix='%',
                gridcolor='rgba(255, 255, 255, 0.2)'
            ),
            angularaxis=dict(
                tickfont=dict(color='white', size=14),
                gridcolor='rgba(255, 255, 255, 0.2)'
            ),
            bgcolor='rgba(0, 0, 0, 0)'
        ),
        paper_bgcolor='rgba(0, 0, 0, 0)',
        font=dict(color='white'),
        margin=dict(t=30, b=30)
    )
    
    return fig


def analyze_response_patterns(df: pd.DataFrame, name: str) -> dict:
    """ëŒ€í™” ì‘ë‹µ íŒ¨í„´ ë¶„ì„"""
    try:
        # ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„
        df = df.sort_values('timestamp')
        user_msgs = df[df['name'] == name]
        
        if len(user_msgs) == 0:
            return {"error": "ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„ì„
        hour_dist = user_msgs['timestamp'].dt.hour.value_counts()
        peak_hours = hour_dist[hour_dist >= hour_dist.mean()].index.tolist()
        
        # ì‘ë‹µ ì‹œê°„ ë¶„ì„
        df['time_diff'] = df['timestamp'].diff().dt.total_seconds()
        user_responses = df[
            (df['name'] == name) & 
            (df['time_diff'].notna()) & 
            (df['time_diff'] > 0) & 
            (df['time_diff'] <= 3600)  # 1ì‹œê°„ ì´ë‚´ ì‘ë‹µë§Œ
        ]
        
        # ë¹ ë¥¸ ì‘ë‹µ (1ë¶„ ì´ë‚´)
        quick_responses = user_responses[user_responses['time_diff'] <= 60]
        # ì ë‹¹í•œ ì‘ë‹µ (1ë¶„~5ë¶„)
        medium_responses = user_responses[(user_responses['time_diff'] > 60) & (user_responses['time_diff'] <= 300)]
        # ëŠë¦° ì‘ë‹µ (5ë¶„~1ì‹œê°„)
        slow_responses = user_responses[user_responses['time_diff'] > 300]

        # ì—°ì† ëŒ€í™” ë¶„ì„
        consecutive_msgs = 0
        max_consecutive = 0
        current_consecutive = 0
        prev_name = None
        
        for name_current in df['name']:
            if name_current == name:
                if prev_name == name:
                    current_consecutive += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                else:
                    current_consecutive = 1
                consecutive_msgs += 1
            prev_name = name_current

        # ëŒ€í™” ì‹œì‘ ë¹„ìœ¨ (ì²« ë°œí™”)
        conversation_gaps = df['time_diff'] > 1800  # 30ë¶„ ì´ìƒ ê°„ê²©ì„ ìƒˆ ëŒ€í™”ë¡œ ê°„ì£¼
        new_conversations = df[conversation_gaps].index
        conversation_starts = sum(1 for idx in new_conversations if df.loc[idx, 'name'] == name)

        # ë©”ì‹œì§€ ê¸¸ì´ íŒ¨í„´
        msg_lengths = user_msgs['message'].str.len()
        
        response_patterns = {
            "ì‘ë‹µ_ì†ë„": {
                "ë¹ ë¥¸_ì‘ë‹µ_ë¹„ìœ¨": round(len(quick_responses) / max(1, len(user_responses)) * 100, 1),
                "í‰ê· _ì‘ë‹µ_ì‹œê°„": round(user_responses['time_diff'].mean() / 60, 1),  # ë¶„ ë‹¨ìœ„
                "1ë¶„ë‚´_ì‘ë‹µ": len(quick_responses),
                "5ë¶„ë‚´_ì‘ë‹µ": len(medium_responses),
                "1ì‹œê°„ë‚´_ì‘ë‹µ": len(slow_responses)
            },
            "ëŒ€í™”_íŒ¨í„´": {
                "ì£¼ìš”_í™œë™_ì‹œê°„": sorted(peak_hours),
                "ì¼í‰ê· _ë©”ì‹œì§€": round(len(user_msgs) / df['timestamp'].dt.date.nunique(), 1),
                "ì—°ì†_ë°œí™”_ìµœëŒ€": max_consecutive,
                "ëŒ€í™”_ì‹œì‘_íšŸìˆ˜": conversation_starts
            },
            "ë©”ì‹œì§€_ê¸¸ì´": {
                "í‰ê· _ê¸¸ì´": round(msg_lengths.mean(), 1),
                "ìµœëŒ€_ê¸¸ì´": int(msg_lengths.max()),
                "ì§§ì€_ë©”ì‹œì§€_ë¹„ìœ¨": round(sum(msg_lengths < 10) / len(msg_lengths) * 100, 1),
                "ê¸´_ë©”ì‹œì§€_ë¹„ìœ¨": round(sum(msg_lengths > 50) / len(msg_lengths) * 100, 1)
            },
            "ëŒ€í™”_íŠ¹ì„±": analyze_conversation_characteristics(df, name)
        }

        return response_patterns
        
    except Exception as e:
        return {"error": str(e)}

def analyze_conversation_characteristics(df: pd.DataFrame, name: str) -> dict:
    """ìƒì„¸í•œ ëŒ€í™” íŠ¹ì„± ë¶„ì„"""
    user_msgs = df[df['name'] == name]['message'].astype(str)
    
    # ëŒ€í™” íŠ¹ì„± íŒ¨í„´
    patterns = {
        "ì§ˆë¬¸_í•˜ê¸°": r'\?|ì–´ë•Œ|í• ê¹Œ|ë­|ì–¸ì œ|ì–´ë””|ëˆ„êµ¬',
        "ê°ì •_í‘œí˜„": r'[ã…‹ã…ã… ã…œ]{2,}|[ğŸ˜ŠğŸ˜„ğŸ˜¢ğŸ˜­ğŸ˜¡â¤ï¸ğŸ‘]',
        "ë™ì˜_í‘œí˜„": r'ê·¸ë˜|ë§ì•„|ì‘|ã…‡ã…‡|ì•Œê² |ë‹¹ì—°',
        "ì¡´ì¹­_ì‚¬ìš©": r'ìš”|ë‹ˆë‹¤|ì„¸ìš”|ì‹œ|ê»˜ì„œ|ë“œë¦½',
        "ê°•ì¡°_í‘œí˜„": r'!+|ì§„ì§œ|ëŒ€ë°•|ì™„ì „|ë„ˆë¬´|ì •ë§',
        "ë§í¬_ê³µìœ ": r'http|www|com|net|kr|youtube',
        "ì´ëª¨í‹°ì½˜": r'\[ì´ëª¨í‹°ì½˜\]|ğŸ˜Š|ğŸ˜„|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|â¤ï¸|ğŸ‘'
    }
    
    characteristics = {}
    total_msgs = len(user_msgs)
    
    for name, pattern in patterns.items():
        matches = sum(1 for msg in user_msgs if re.search(pattern, msg))
        ratio = round((matches / total_msgs) * 100, 1)
        characteristics[name] = ratio
        
    # ëŒ€í™” ìŠ¤íƒ€ì¼ ê²°ì •
    style_score = {
        "ì¹œê·¼ë„": characteristics.get("ê°ì •_í‘œí˜„", 0) + characteristics.get("ì´ëª¨í‹°ì½˜", 0),
        "ì ê·¹ì„±": characteristics.get("ì§ˆë¬¸_í•˜ê¸°", 0) + characteristics.get("ê°•ì¡°_í‘œí˜„", 0),
        "ê³µì†ë„": characteristics.get("ì¡´ì¹­_ì‚¬ìš©", 0),
        "ì •ë³´ì„±": characteristics.get("ë§í¬_ê³µìœ ", 0),
        "ë°˜ì‘ì„±": characteristics.get("ë™ì˜_í‘œí˜„", 0)
    }
    
    main_style = max(style_score.items(), key=lambda x: x[1])
    
    return {
        "ì£¼ìš”_íŠ¹ì„±": {k: v for k, v in characteristics.items() if v > 20},
        "ëŒ€í™”_ìŠ¤íƒ€ì¼": main_style[0],
        "ìŠ¤íƒ€ì¼_ì ìˆ˜": round(main_style[1], 1)
    }

def analyze_reaction_speed(df: pd.DataFrame, name: str) -> dict:
    """ë°˜ì‘ ì†ë„ ìƒì„¸ ë¶„ì„"""
    df = df.sort_values('timestamp')
    df['time_diff'] = df['timestamp'].diff().dt.total_seconds()
    
    user_responses = df[
        (df['name'] == name) & 
        (df['time_diff'].notna()) & 
        (df['time_diff'] > 0)
    ]
    
    response_times = user_responses['time_diff'].values
    
    if len(response_times) == 0:
        return {
            "error": "ì‘ë‹µ ì‹œê°„ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    speed_categories = {
        "ì¦‰ê°_ì‘ë‹µ": sum(response_times <= 30),  # 30ì´ˆ ì´ë‚´
        "ë¹ ë¥¸_ì‘ë‹µ": sum((response_times > 30) & (response_times <= 60)),  # 1ë¶„ ì´ë‚´
        "ë³´í†µ_ì‘ë‹µ": sum((response_times > 60) & (response_times <= 300)),  # 5ë¶„ ì´ë‚´
        "ëŠë¦°_ì‘ë‹µ": sum((response_times > 300) & (response_times <= 3600)),  # 1ì‹œê°„ ì´ë‚´
        "ë§¤ìš°_ëŠë¦°_ì‘ë‹µ": sum(response_times > 3600)  # 1ì‹œê°„ ì´ˆê³¼
    }
    
    total_responses = len(response_times)
    speed_ratios = {
        k: round(v / total_responses * 100, 1)
        for k, v in speed_categories.items()
    }
    
    # ì‘ë‹µ ì†ë„ íŠ¹ì„± ë¶„ì„
    avg_speed = np.mean(response_times)
    if avg_speed <= 60:
        speed_characteristic = "ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µì"
    elif avg_speed <= 300:
        speed_characteristic = "ì‹ ì†í•œ ì‘ë‹µì"
    elif avg_speed <= 900:
        speed_characteristic = "ë³´í†µ ì‘ë‹µì"
    else:
        speed_characteristic = "ì—¬ìœ ë¡œìš´ ì‘ë‹µì"
    
    return {
        "ì‘ë‹µ_ë¶„í¬": speed_ratios,
        "í‰ê· _ì‘ë‹µ_ì‹œê°„": round(avg_speed / 60, 1),  # ë¶„ ë‹¨ìœ„
        "ì‘ë‹µ_íŠ¹ì„±": speed_characteristic,
        "ìµœì†Œ_ì‘ë‹µ_ì‹œê°„": round(np.min(response_times), 1),
        "ìµœëŒ€_ì‘ë‹µ_ì‹œê°„": round(np.min(response_times) / 60, 1)  # ë¶„ ë‹¨ìœ„
    }

def calculate_charm_score(messages: list) -> float:
    """ë§¤ë ¥ë„ ì ìˆ˜ ê³„ì‚°"""
    charm_patterns = {
        'ì„¼ìŠ¤': {
            'patterns': [
                r'ã…‹ã…‹|ã…ã…|ì›ƒê¸´|ì¬ë¯¸|ì‹ ê¸°|ë©‹ì§€',
                r'êµ¿|ì¢‹ì•„|ìµœê³ |ëŒ€ë°•|ì˜¤|ì™€',
                r'ã„·ã„·|ì˜¤ìš°|wow|í—|ë¯¸ì³¤|ì‹¤í™”'
            ],
            'weight': 1.2
        },
        'ë°°ë ¤': {
            'patterns': [
                r'ê´œì°®|ë„ì™€|í•¨ê»˜|ë¶€íƒ|í™•ì¸',
                r'ë¯¸ì•ˆ|ê°ì‚¬|ê³ ë§ˆì›Œ|ì£„ì†¡|ë§ì”€',
                r'ì¡°ì‹¬|ê±´ê°•|ê±±ì •|ì±™ê²¨|ë³´ì‚´'
            ],
            'weight': 1.3
        },
        'ì ê·¹ì„±': {
            'patterns': [
                r'í•˜ì|ê°€ì|ì¢‹ì§€|ë‹¹ì—°|ì˜¤ì¼€ì´',
                r'ê·¸ë˜|í•´ë³´|ì§„í–‰|ì‹œë„|ë„ì „',
                r'ì™„ì „|ì§„ì§œ|ë„ˆë¬´|ì •ë§|ë§¤ìš°'
            ],
            'weight': 1.1
        },
        'ê°ì„±': {
            'patterns': [
                r'ì˜ˆì˜|ë©‹ì§€|ê·€ì—½|ì‚¬ë‘|ì¢‹ì•„',
                r'í–‰ë³µ|ê·¸ë¦½|ë³´ê³ ì‹¶|ì•„ë¦„|ì„¤ë ˆ',
                r'â¤ï¸|ğŸ’•|ğŸ˜Š|ğŸ¥°|ğŸ˜'
            ],
            'weight': 1.0
        },
        'ìœ ë¨¸': {
            'patterns': [
                r'ã…‹ã…‹ã…‹|ã…ã…ã…|ì›ƒê²¨|ì¬ë°Œ|ì›ƒê¹€',
                r'ë“œë¦½|ê°œê·¸|ë†ë‹´|ì¥ë‚œ|ìœ ë¨¸',
                r'í‚¹|ì§±|ëŒ€ë°•|ì—­ëŒ€ê¸‰|ë ˆì „ë“œ'
            ],
            'weight': 1.2
        }
    }
    
    return calculate_weighted_pattern_score(messages, charm_patterns)

def calculate_friendship_score(messages: list) -> float:
    """ì¹œí™”ë ¥ ì ìˆ˜ ê³„ì‚°"""
    friendship_patterns = {
        'ê³µê°': {
            'patterns': [
                r'ë§ì•„|ê·¸ë˜|ì´í•´|ë‹¹ì—°|ì‹¬í•˜',
                r'ê°™ì€|ë¹„ìŠ·|ì—­ì‹œ|ê·¸ìµ¸|ë™ê°',
                r'ë‚˜ë„|ì €ë„|ìš°ë¦¬ë„|ë§ˆì°¬|ì²˜ëŸ¼'
            ],
            'weight': 1.3
        },
        'ê´€ì‹¬': {
            'patterns': [
                r'ì–´ë•Œ|ê´œì°®|ìš”ì¦˜|ê·¼ë°|ë¬´ìŠ¨',
                r'ì™œ|ë­|ëˆ„êµ¬|ì–¸ì œ|ì–´ë””',
                r'í˜¹ì‹œ|ìˆì–´|ì—†ì–´|ê°™ì€|ë³´ë‹ˆ'
            ],
            'weight': 1.1
        },
        'ì§€ì§€': {
            'patterns': [
                r'í˜ë‚´|í™”ì´íŒ…|ì‘ì›|ì¢‹ê² |ê¸°ëŒ€',
                r'ëŒ€ë‹¨|ì¶•í•˜|ìˆ˜ê³ |ê³ ìƒ|ì˜í–ˆ',
                r'ë¯¿ì–´|ì•„ì|íŒŒì´íŒ…|êµ¿|ìµœê³ '
            ],
            'weight': 1.2
        },
        'ì¹œê·¼': {
            'patterns': [
                r'ì•¼|ì—¬ê¸°|ì €ê¸°|ë„ˆ|ë‚˜',
                r'ìš°ë¦¬|ê°™ì´|ë‘˜ì´|ë‹¤ê°™|í•¨ê»˜',
                r'ì¹œêµ¬|ë™ìƒ|ì–¸ë‹ˆ|ì˜¤ë¹ |í˜•'
            ],
            'weight': 1.0
        }
    }
    
    return calculate_weighted_pattern_score(messages, friendship_patterns)

def calculate_wit_score(messages: list) -> float:
    """ì¬ì¹˜ë ¥ ì ìˆ˜ ê³„ì‚°"""
    wit_patterns = {
        'ìˆœë°œë ¥': {
            'patterns': [
                r'ë°”ë¡œ|ë‹¹ì—°|ì—­ì‹œ|ì¿¨|êµ¿|ì˜¤ì¼€',
                r'ì¦‰ì‹œ|ë¹¨ë¦¬|ê¸ˆë°©|ìˆœì‹|ì¬ë¹¨',
                r'ì²™ì²™|ë²ˆê°œ|ìˆœê°„|ì¦‰ê°|ë°”ëŒ'
            ],
            'weight': 1.2
        },
        'ì„¼ìŠ¤': {
            'patterns': [
                r'ì„¼ìŠ¤|ê¹”ë”|ë”±|ì°°ë–¡|ì™„ë²½',
                r'ê¸°ê°€|ê°“|ë‡Œì„¹|ì²œì¬|í˜„ì‹¤',
                r'ì˜ˆë¦¬|ë˜‘ë˜‘|ì˜ë¦¬|ëª…ì¾Œ|íƒì›”'
            ],
            'weight': 1.3
        },
        'ìœ ë¨¸': {
            'patterns': [
                r'ã…‹ã…‹|ì›ƒê¸´|ì¬ë¯¸|ë“œë¦½|ì›ƒìŒ',
                r'ì¥ë‚œ|ë†ë‹´|ê°œê·¸|ì½”ë¯¸|ì§–ê¶ƒ',
                r'í‹°í‚¤|íƒ€í‚¤|ê¾¸ë¥´|ì¼ë¯¼|ë ˆì „'
            ],
            'weight': 1.1
        }
    }
    
    return calculate_weighted_pattern_score(messages, wit_patterns)

def calculate_emotion_score(messages: list) -> float:
    """ê°ì„±ë ¥ ì ìˆ˜ ê³„ì‚°"""
    emotion_patterns = {
        'ê°ì •í‘œí˜„': {
            'patterns': [
                r'ì¢‹ì•„|í–‰ë³µ|ê¸°ì˜|ìŠ¬í”„|í™”ë‚˜',
                r'ê·¸ë¦½|ì„¤ë ˆ|ê¶ê¸ˆ|ë‹µë‹µ|íë­‡',
                r'ğŸ˜Š|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|ğŸ¥°'
            ],
            'weight': 1.3
        },
        'ê³µê°': {
            'patterns': [
                r'ì´í•´|ë§ì•„|ê·¸ë ‡|ë‹¹ì—°|ê°™ì´',
                r'ëŠë‚Œ|ì„œë¡œ|ìš°ë¦¬|í•¨ê»˜|ê³µê°',
                r'ì•„ë‹ˆ|ì—­ì‹œ|ì§„ì§œ|ì •ë§|ë„ˆë¬´'
            ],
            'weight': 1.2
        },
        'ë°°ë ¤': {
            'patterns': [
                r'ê´œì°®|ë¯¸ì•ˆ|ê°ì‚¬|ë¶€íƒ|ê±±ì •',
                r'ì¡°ì‹¬|ê±´ê°•|í˜ë‚´|ìœ„ë¡œ|ë„ì™€',
                r'ì£¼ì˜|ë³´ì‚´|ì±™ê¸°|ì‹ ê²½|ë°°ë ¤'
            ],
            'weight': 1.1
        }
    }
    
    return calculate_weighted_pattern_score(messages, emotion_patterns)

def calculate_interest_score(messages: list) -> float:
    """ê´€ì‹¬ì‚¬ ì§‘ì¤‘ë„ ì ìˆ˜ ê³„ì‚°"""
    interest_patterns = {
        'ì·¨ë¯¸': {
            'patterns': [
                r'ì¢‹ì•„|ì¬ë¯¸|ìµœê³ |ëŒ€ë°•|ì§±',
                r'ì·¨ë¯¸|ê´€ì‹¬|ì—´ì •|ëª°ì…|ë¹ ì ¸',
                r'ì¦ê¸°|ë†€ê¸°|ë†€ëŸ¬|ì²´í—˜|ê²½í—˜'
            ],
            'weight': 1.2
        },
        'í•™ìŠµ': {
            'patterns': [
                r'ê³µë¶€|ë°°ìš°|ì—°êµ¬|í•™ìŠµ|ê°•ì˜',
                r'ì±…|ë…ì„œ|ìê²©|ì‹œí—˜|ê³¼ì •',
                r'ì •ë³´|ì§€ì‹|ì´ë¡ |ì‹¤ì „|ì‹¤ìŠµ'
            ],
            'weight': 1.1
        },
        'ë¬¸í™”': {
            'patterns': [
                r'ì˜í™”|ë“œë¼ë§ˆ|ìŒì•…|ê³µì—°|ì „ì‹œ',
                r'ê²Œì„|ë§Œí™”|ì›¹íˆ°|ì†Œì„¤|ì±…',
                r'ì˜ˆìˆ |ë¬¸í™”|ê³µì—°|ì‘í’ˆ|ê°ìƒ'
            ],
            'weight': 1.0
        }
    }
    
    return calculate_weighted_pattern_score(messages, interest_patterns)

def calculate_weighted_pattern_score(messages: list, pattern_categories: dict) -> float:
    """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ íŒ¨í„´ ì ìˆ˜ ê³„ì‚°"""
    if not messages:
        return 0
    
    msg_text = ' '.join(str(m) for m in messages if isinstance(m, str))
    total_score = 0
    total_weight = 0
    
    for category, info in pattern_categories.items():
        category_score = 0
        patterns = info['patterns']
        weight = info['weight']
        
        for pattern_group in patterns:
            matches = sum(len(re.findall(p, msg_text, re.IGNORECASE)) 
                         for p in pattern_group.split('|'))
            # íŒ¨í„´ ê·¸ë£¹ë‹¹ ìµœëŒ€ ì ìˆ˜ ì œí•œ
            category_score += min(100, matches * 5)
        
        # ì¹´í…Œê³ ë¦¬ í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_category_score = category_score / len(patterns)
        total_score += avg_category_score * weight
        total_weight += weight
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚° (0-100 ë²”ìœ„ë¡œ ì •ê·œí™”)
    final_score = (total_score / total_weight) if total_weight > 0 else 0
    return round(min(100, final_score), 1)


def main():
    st.title("ğŸ’¬ ì¹´í†¡ ëŒ€í™” ë¶„ì„ê¸°")
    st.markdown("### AIê°€ ì—¬ëŸ¬ë¶„ì˜ ì¹´í†¡ë°©ì„ ë¶„ì„í•´ë“œë ¤ìš”! ğŸ¤–")

    with st.sidebar:
        st.markdown("""
        ### ğŸ“± ì‚¬ìš© ë°©ë²•
        1. ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ì°½ â†’ ë©”ë‰´
        2. ëŒ€í™”ë‚´ìš© ë‚´ë³´ë‚´ê¸° (.txt)
        3. íŒŒì¼ ì—…ë¡œë“œ
        4. ë¶„ì„ ì‹œì‘!
        
        ### ğŸ” ë¶„ì„ í•­ëª©
        - ëŒ€í™” ë§¥ë½ê³¼ ê´€ê³„
        - ì£¼ì œë³„ ë¶„ì„
        - ê°ì • ë¶„ì„
        - ëŒ€í™” íŒ¨í„´
        - GPT ì‹¬ì¸µ ë¶„ì„
        - ì„±ê²© ë¶„ì„
        """)

    # íŒŒì¼ ì—…ë¡œë“œ
    chat_file = st.file_uploader(
        "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ë³´ë‚´ê¸° íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš” (.txt)",
        type=['txt']
    )
    
    if chat_file:
        with st.spinner("ëŒ€í™” ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            chat_text = chat_file.read().decode('utf-8')
            df = parse_kakao_chat(chat_text)
        
        if len(df) > 0:
            unique_names = df['name'].unique()
            
            # ì°¸ì—¬ì ì„ íƒ
            col1, col2 = st.columns(2)
            with col1:
                my_name = st.selectbox(
                    "ë¶„ì„í•  ëŒ€í™”ë°©ì˜ ë‹¹ì‹  ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”",
                    options=unique_names
                )
            with col2:
                target_names = st.multiselect(
                    "ë¶„ì„í•  ëŒ€ìƒì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ëª… ì„ íƒ ê°€ëŠ¥)",
                    options=[n for n in unique_names if n != my_name],
                    default=[n for n in unique_names if n != my_name]
                )
            
            if st.button("ğŸ” ëŒ€í™” ë¶„ì„ ì‹œì‘", use_container_width=True):
                with st.spinner("AIê°€ ëŒ€í™”ë¥¼ ì‹¬ì¸µ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤..."):
                    # 1. ê¸°ë³¸ í†µê³„
                    st.markdown("## ğŸ“Š ëŒ€í™”ë°© ê¸°ë³¸ í†µê³„")
                    date_range = (df['timestamp'].max() - df['timestamp'].min())
                    total_duration = date_range.days + 1  # ìµœì†Œ 1ì¼
                    unique_dates = len(df['timestamp'].dt.date.unique())
                    total_messages = len(df)

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(
                            "ì´ ëŒ€í™” ê¸°ê°„",
                            f"{unique_dates}ì¼",
                            f"ì „ì²´ {total_duration}ì¼ ì¤‘"
                        )
                    with col2:
                        daily_avg = total_messages / unique_dates if unique_dates > 0 else total_messages
                        st.metric(
                            "ì´ ë©”ì‹œì§€ ìˆ˜",
                            f"{total_messages:,}ê°œ",
                            f"í•˜ë£¨ í‰ê·  {daily_avg:.1f}ê°œ"
                        )
                    with col3:
                        active_users = len([n for n in unique_names 
                                        if len(df[df['name']==n]) > total_messages*0.1])
                        st.metric(
                            "ì°¸ì—¬ì ìˆ˜",
                            f"{len(unique_names)}ëª…",
                            f"í™œì„± ì‚¬ìš©ì {active_users}ëª…"
                        )
                    
                    # 2. GPT ëŒ€í™” ë¶„ì„
                    st.markdown("## ğŸ¤– AI ëŒ€í™” ë¶„ì„")
                    analysis = analyze_chat_context(df, target_names, my_name)
                    
                    if analysis:
                        tabs = st.tabs(["ğŸ’¡ ì „ë°˜ì  ë¶„ì„", "ğŸ‘¥ ê´€ê³„ ë¶„ì„", "ğŸ“Š ì£¼ì œ ë¶„ì„", "ğŸ“ˆ íŒ¨í„´ ë¶„ì„"])
                        
                        with tabs[0]:
                            if 'gpt_analysis' in analysis:
                                st.markdown(analysis['gpt_analysis'])
                            else:
                                st.info("GPT ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        with tabs[1]:
                            st.markdown("### ì°¸ì—¬ì ê´€ê³„ë„")
                            if 'relationships' in analysis:
                                st.plotly_chart(
                                    create_relationship_graph(analysis['relationships']),
                                    use_container_width=True
                                )
                            else:
                                st.info("ê´€ê³„ ë¶„ì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        with tabs[2]:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'topics' in analysis:
                                    st.plotly_chart(
                                        create_topic_chart(analysis['topics']),
                                        use_container_width=True
                                    )
                                else:
                                    st.info("ì£¼ì œ ë¶„ì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            with col2:
                                st.markdown("### ì£¼ìš” í‚¤ì›Œë“œ")
                                wordcloud_fig = create_wordcloud(df['message'])
                                if wordcloud_fig:
                                    st.pyplot(wordcloud_fig)
                                else:
                                    st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        with tabs[3]:
                            st.markdown("### ì‹œê°„ëŒ€ë³„ ëŒ€í™” íŒ¨í„´")
                            st.plotly_chart(
                                create_time_pattern(df, target_names, my_name),
                                use_container_width=True
                            )
                    
                    # 3. ì„±ê²© ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€ëœ ì„¹ì…˜)
                    st.markdown("## ğŸ­ ì„±ê²© ë¶„ì„")
                    display_personality_analysis(df, target_names)
                    
                    # 4. ìƒì„¸ ë¶„ì„
                    st.markdown("## ğŸ“± ìƒì„¸ ëŒ€í™” ë¶„ì„")
                    
                    # ëŒ€í™”ëŸ‰ ë¶„ì„
                    st.markdown("### ğŸ’¬ ì°¸ì—¬ìë³„ ëŒ€í™”ëŸ‰")
                    conversation_stats = analyze_conversation_stats(df)
                    st.plotly_chart(
                        create_conversation_chart(conversation_stats),
                        use_container_width=True
                    )
                    
                    # ê°ì • ë¶„ì„
                    st.markdown("### ğŸ˜Š ê°ì • ë¶„ì„")
                    col1, col2 = st.columns(2)
                    with col1:
                        emotion_stats = analyze_emotions(df)
                        st.plotly_chart(
                            create_emotion_chart(emotion_stats),
                            use_container_width=True
                        )
                    with col2:
                        st.markdown("#### ì£¼ìš” ê°ì • í‚¤ì›Œë“œ")
                        try:
                            emotion_cloud = create_detailed_wordcloud(df['message'])
                            if emotion_cloud:
                                st.pyplot(emotion_cloud)
                            else:
                                st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë¶„ì„ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
                                # ëŒ€ì²´ ë¶„ì„: ìƒìœ„ ê°ì • í‚¤ì›Œë“œ í‘œì‹œ
                                emotions = df['message'].str.findall(r'[ã…‹ã…]{2,}|[ã… ã…œ]{2,}|[!?]{2,}|ğŸ˜Š|ğŸ˜„|ğŸ˜¢|ğŸ˜­|ğŸ˜¡|â¤ï¸|ğŸ‘|ğŸ™').explode()
                                top_emotions = emotions.value_counts().head(10)
                                if not top_emotions.empty:
                                    st.write("ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ê°ì • í‘œí˜„:")
                                    for emotion, count in top_emotions.items():
                                        st.write(f"- {emotion}: {count}íšŒ")
                                else:
                                    st.write("ê°ì • í‘œí˜„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            st.error(f"ê°ì • ë¶„ì„ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    
                    # ëŒ€í™” í•˜ì´ë¼ì´íŠ¸
                    st.markdown("## âœ¨ ëŒ€í™” í•˜ì´ë¼ì´íŠ¸")
                    highlights = find_highlight_messages(df, target_names, my_name)
                    
                    tabs = st.tabs(["ğŸ’ ì¸ìƒì ì¸ ëŒ€í™”", "ğŸš€ í™œë°œí•œ í† ë¡ ", "âš¡ ë¹ ë¥¸ ë‹µì¥"])
                    with tabs[0]:
                        if highlights and 'emotional_messages' in highlights:
                            for msg in highlights['emotional_messages']:
                                st.info(f"{msg['timestamp'].strftime('%Y-%m-%d %H:%M')} - {msg['name']}: {msg['message']}")
                        else:
                            st.write("ì¸ìƒì ì¸ ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    with tabs[1]:
                        if highlights and 'discussion_messages' in highlights:
                            for msg in highlights['discussion_messages']:
                                st.info(f"{msg['timestamp'].strftime('%Y-%m-%d %H:%M')} - {msg['name']}: {msg['message']}")
                        else:
                            st.write("í™œë°œí•œ í† ë¡ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    with tabs[2]:
                        if highlights and 'quick_responses' in highlights:
                            for msg in highlights['quick_responses']:
                                st.info(f"{msg['timestamp'].strftime('%Y-%m-%d %H:%M')} - {msg['name']}: {msg['message']}")
                        else:
                            st.write("ë¹ ë¥¸ ë‹µì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # 5. AI ì œì•ˆ
                    display_suggestions(analysis)
                                        
        else:
            st.error("ì±„íŒ… ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # Footer
    st.markdown("---")
    st.markdown("Made with â¤ï¸ by AI")

if __name__ == "__main__":
    main()